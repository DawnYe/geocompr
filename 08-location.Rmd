# Location analysis {#location}

## Prerequisites

```{r, message = FALSE} 
library(sf)
library(sp)
library(ggmap)
library(raster)
library(osmdata)
library(tidyverse)
```

## Introduction

This chapter demonstrates how the skills learned in the previous chapters can be applied to location analysis.
Location analysis is a broad field of research and (largely) commercial application, the aim of which is to decide the optimal location for interventions.
A typical example is where to locate a new shop: there are typically hundreds of possible locations in a city and location analysis can be used to decide where the shop will attract most visitors and, ultimately, make most profit.
There are also many non-commercial applications that can use the technique for public benefit.
A good example is where to locate new health services benefiting as many people as possible [@tomintz_geography_2008].

Ecological concepts can be useful in understanding location analysis.
Animals have needs which they tend to meet by finding the optimal location based on variables that change over space (called gradients in the ecological literature --- see chapter xx).
<!-- add reference!! -->
A polar bear, for example, may prefer northern latitudinal regions where the ice is persistent throughout the summer, and where food (seals and sea lions) and shelter are plentiful.
The further the bear lives from such conditions, the less likely is its survival.

Similarly, a service can have an economic niche.
There are locations of low and high turnovers, based on a range of variables that change over geographic space.
The main task of geomarketing is to answer the question: where are optimal locations for services based on available data?
Typical geomarketing sub-questions include:

- Where are the service users (or clients), specific target groups and competitors?
- How many people can easily reach my stores?
- Do existing services over or under-exploit the market potential?
- How big is my market share and turnovers, and are there spatial differences?

## Use case

Imagine your are launching a cycling company and would like to open shops across urban areas of Germany.
Survey data reveals who the current clients are: males between 20-40 years old who live alone or with just one person (single households, not families).
In this hypothetical example the following procedure could be used to find suitable locations for the stores:

<!-- just internal structure and brain storming -->
- download German census data and convert it into a raster (resolution: 1 km^2^)
- find metropolitan areas, i.e. aggregate to a resolution of 10 sq-km (define a threshold for metropolitan areas, and point out that there are still more advanced ways to do so)
- download OSM, find bike shops + other interesting POIs.
Create a POI raster (= attraction raster)
- restrict to metropolitan areas
- weights:
  - reclassify age raster: 16-40 weight = 2; all other weight = 1
  - reclassify gender raster:
  - reclassify household raster
	- POIs: reclassify into 5 classes, the more POIs, the better
	- finally map algebra: age + gender + POI raster will show the most favorable locations
- Exercise: download the 100 m census inhabitant data and convert it into a raster (will be used in the chapter continuing location analysis)
  

## Create census rasters

The German government provides a csv-file containing gridded census data of 2011 for Germany.
One can either download a 1 km or a 100 m resolution.
Here, we download the 1 km data, unzip it and read it into R.

```{r, eval = FALSE}
url = paste0("https://www.zensus2011.de/SharedDocs/Downloads/DE/", 
             "Pressemitteilung/DemografischeGrunddaten/csv_Zensusatlas_", 
             "klassierte_Werte_1km_Gitter.zip?__blob=publicationFile&v=8")
download.file(url = url, destfile = file.path(tempdir(), "census.zip"),
              method = "auto", mode = "wb")
# list the file names
nms = unzip(file.path(tempdir(), "census.zip"), list = TRUE)
# unzip only the csv file
base_name = grep(".csv$", nms$Name, value = TRUE)
unzip(file.path(tempdir(), "census.zip"), files = base_name, exdir = tempdir())
# read in the csv file
input = readr::read_csv2(file.path(tempdir(), base_name))
```

Next we select the variables we need: x and y coordinates, the number of inhabitants (population), mean age, portion of women and the household size (Table \@ref(tab:census-desc)). 

```{r census-desc, echo = FALSE}
tab = tribble(
  ~"class", ~"pop", ~"women", ~"age", ~"hh",
  1, "3-250", "0-40", "0-40", "1-2", 
  2, "250-500", "40-47", "40-42", "2-2.5",
  3, "500-2000", "47-53", "42-44", "2.5-3",
  4, "2000-4000", "53-60", "44-47", "3-3.5",
  5, "4000-8000", ">60", ">47", ">3.5",
  6, ">8000", "", "", ""
)
cap = paste("Excerpt from the data description",
             "'Datensatzbeschreibung_klassierte_Werte_1km-Gitter.xlsx'", 
             "located in the downloaded file census.zip describing the classes", 
             "of the retained variables. The classes -1 and -9 refer to", 
             "uninhabited areas or are areas which have to be kept secret", 
             "for example due to anonymization reasons.")
knitr::kable(tab,
             col.names =  c("class", "population\\\n(number of persons)",
                            "women\\\n(%)", "mean age\\\n(years)",
                            "household size\\\n(number of persons)"),
             caption = cap, align = "c", format = "html")
```

Additionally, we set the values -1 and -9 to `NA` since these values are unknown.

```{r, eval = FALSE}
# pop = population, hh_size = household size
input = dplyr::select(input, x = x_mp_1km, y = y_mp_1km, pop = Einwohner,
                      women = Frauen_A, mean_age = Alter_D,
                      hh_size = HHGroesse_D)
# set -1 and -9 to NA
input = mutate_all(input, funs(ifelse(. %in% c(-1, -9), NA, .)))
```
 
After the data preprocessing, we convert the table into a raster stack.
First, we use the x- and y-coordinates to convert the `data.frame` into a `SpatialPolygonsDataFrame` (we do not use sf-objects here, since the **raster** package does not support `sf`).
`gridded()` converts a `SpatialPolygonsDataFrame` into a `SpatialPixelsDataFrame` which in turn can be used as an input for **raster**'s `stack()` function.
The final raster stack consists of four layers: inhabitants, mean age, portion of women and household size (Fig. \@ref(fig:census-stack)).

```{r, eval = FALSE}
# convert table into a raster (x and y are cell midpoints)
coordinates(input) =~ x + y
# use the correct projection (see data description)
proj4string(input) = CRS("+init=epsg:3035")
gridded(input) = TRUE
# convert into a raster stack
input = stack(input)
```

```{r census-stack, echo = FALSE, fig.cap = "Gridded German census data of 2011. See Table \\@ref(tab:census-desc) for a description of the classes."}
knitr::include_graphics("figures/08_census_stack.png")
```

<!-- find out about new lines in headings + blank cells-->
We `reclassify()` the ordinal-scaled rasters in accordance with our survey (see section \@ref(use-case)).
In the case of the population data we convert the classes into a numeric data type using class means. 
This means we use 127 for class 1 (ranging from 3 to 250 inhabitants), 375 for class 2 (ranging from 250 to 500 inhabitants), etc. (Table \@ref(tab:census-desc)).
We arbitrarily chose 8000 inhabitants for class 6 since it represents cells where more than 8000 persons live.
Of course, this is only a simple approximation of the true population number (in the exercises you can explore the introduced error in more detail), however, for our purpose of delineating metropolitan areas the level of detail is good enough (see next section).
By contrast, we convert the remaining input rasters into weight rasters whereas the highest weights are in correspondence with our survey.
For instance, the women class 1 (0-40%) receives a weight of 3 since our clientele is predominantly male.
Equally, the mean age class containing the youngest people receives the highest weight as well as the single households class.

```{r, eval = FALSE}
rcl_pop = matrix(c(1, 1, 127, 2, 2, 375, 3, 3, 1250, 4, 4, 3000, 5, 5, 6000,
                   6, 6, 8000), ncol = 3, byrow = TRUE)
rcl_women = matrix(c(1, 1, 3, 2, 2, 2, 3, 3, 1, 4, 5, 0), 
                   ncol = 3, byrow = TRUE)
rcl_age = matrix(c(1, 1, 3, 2, 2, 0, 3, 5, 0),
                 ncol = 3, byrow = TRUE)
rcl_hh = rcl_women
rcl = list(rcl_pop, rcl_women, rcl_age, rcl_hh)
reclass = input
for (i in seq_len(nlayers(reclass))) {
  reclass[[i]] = reclassify(reclass[[i]], rcl = rcl[[i]], right = NA) 
}
names(reclass) = names(input)
```

## Define metropolitan areas

We arbitrarily define a a pixel of 20 sq-km as metropolitan if more than half a million people lives in it.
The `aggregate()` command helps to change the resolution from 1 to 20 sq-km while each output cell represents the sum of all 1 sq-km input cells (see section \@ref(aligning-rasters)).
Next, we only keep cells with more than half a million inhabitants using a Boolean operation, and convert the resulting cells into spatial polygons of class `sf`.

```{r, eval = FALSE}
pop_agg = aggregate(input$pop, fact = 20000 / res(input)[1], fun = sum)
polys = rasterToPolygons(pop_agg[pop_agg > 500000, drop = FALSE])
polys = st_as_sf(polys)
```

Plotting these polygons reveals eight metropolitan regions (Fig. \@ref(fig:metro-areas)).
Each region consists of one ore more polygons (raster cells).
It would be nice if we could join all polygons belonging to one region.
One approach is to dissolve all polygons (see section \@ref(aggregating-or-dissolving)).

```{r, eval = FALSE}
polys = summarize(polys, pop = sum(layer))
```

This returns one multipolygon feature with its elements corresponding to the metropolitan regions. 
To extract these polygons from the multipolygon, we can use `st_cast()`.

```{r, eval = FALSE}
metros = st_cast(polys, "POLYGON")
``` 

The warning message tells us that the attributes are repeated for all sub-geometries.
Here, this means that each region receives the total population of all metropolitan areas combined.
Clearly this is wrong but here we can safely ignore this since we are merely interested in the geometry.

<!-- maybe a good if advanced exercise
This requires finding the nearest neighbors (`st_intersects()`), and some additional processing.
Do not worry too much about the following code.
There is probably a better way to do it. 
Nevertheless, it finds all pixels belonging to one region in a generic way.
We use this information to assign each polygon (pixel) to a region.
Subsequently, we can use the region information to dissolve the pixels into region polygons.

```{r, eval = FALSE}
# dissolve on spatial neighborhood
nbs = st_intersects(polys, polys)
# nbs = over(polys, polys, returnList = TRUE)

fun = function(x, y) {
  tmp = lapply(y, function(i) {
  if (any(x %in% i)) {
   union(x, i)
  } else {
   x
    }
  })
  Reduce(union, tmp)
}
# call function recursively
fun_2 = function(x, y) {
  out = fun(x, y)
  while (length(out) < length(fun(out, y))) {
    out = fun(out, y)
  }
  out
}

cluster = map(nbs, ~ fun_2(., nbs) %>% sort)
# just keep unique clusters
cluster = cluster[!duplicated(cluster)]
# assign the cluster classes to each pixel
for (i in seq_along(cluster)) {
  polys[cluster[[i]], "region_id"] = i
}
# dissolve pixels based on the the region id
polys = group_by(polys, region_id) %>%
  summarize(pop = sum(layer, na.rm = TRUE))
# polys_2 = aggregate(polys, list(polys$region_id), sum)
plot(polys[, "region_id"])

# Another approach, can be also be part of an excercise

coords = st_coordinates(polys_3) %>% 
  as.data.frame
ls = split(coords, f = coords$L2)
ls = lapply(ls, function(x) {
  dplyr::select(x, X, Y) %>%
    as.matrix %>%
    list %>%
    st_polygon
})
metros = do.call(st_sfc, ls)
metros = st_set_crs(metros, 3035)
metros = st_sf(data.frame(region_id = 1:9), geometry = metros)
st_intersects(metros, metros)
plot(metros[-5,])
st_centroid(metros) %>%
  st_coordinates
```
-->

However, visual inspection reveals eight metropolitan areas whereas the dissolving comes up with nine.
This is because one polygon just touches the corner of another polygon (western Germany, Cologne/Düsseldorf area; Fig. \@ref(fig:metro-areas)).

```{r metro-areas, echo = FALSE, fig.width = 1, fig.height = 1, fig.cap = "The aggregated population raster (resolution: 20 km) with the identified metropolitan areas (golden polygons) and the corresponding names."}
knitr::include_graphics("figures/08_metro_areas.png")
```

One could assign it to the neighboring region using another dissolving procedure, however, we leave this as an exercise to the reader, and simply delete the offending polygon.

```{r, eval = FALSE}
# find out about the offending polygon
int = st_intersects(metros, metros)
# polygons 5 and 9 share one border, delete polygon number 5
metros = metros[-5, ]
```

The defined metropolitan areas suitable for bike shops are still missing a name.
A reverse geocoding approach can settle this problem.
Given a coordinate, reverse geocoding finds the corresponding address.
Consequently, extracting the centroid coordinate of each metropolitan area can serve as an input for a reverse geocoding API.
The **ggmap** package makes use of the one provided by Google.^[Note that Google allows each user to access its services on a free basis for a maximum of 2500 queries a day.]
`ggmap::revgeocode()` only accepts geographical coordinates (latitude/longitude), therefore, the first requirement is to bring the metropolitan polygons into an appropriate coordinate reference system (chapter \@ref(transform)).

```{r, eval = FALSE}
# reverse geocoding to find out the names of the metropolitan areas
metros_wgs = st_transform(metros, 4326)
coords = st_centroid(metros_wgs) %>%
  st_coordinates %>%
  round(., 4)
```

Additionally, `ggmap::revgeocode()` only accepts one coordinate at a time, which is why we iterate over each coordinate of `coords` via a loop (`ldply()`).
`ldply()` does exactly the same as `lapply()` except for returning a `data.frame` instead of a `list`.^[To learn more about the split-apply-combine strategy for data analysis, we refer the reader to @wickham_split-apply-combine_2011.]
Sometimes, the reverse geocoding API of Google is unable to find an address returning `NA`.
Often enough trying the same coordinate again, returns an address at the second or third attempt (see `while()-loop`).
However, if three attempts have already failed, this is a good indication that the requested information is indeed unavailable.
Since it is our interest to be a good cyberspace citizen, we try not to overburden the server with too many queries within a short amount of time. 
Instead we let the loop sleep between one and four seconds after each iteration before accessing the reverse geocoding API again.

```{r, eval = FALSE}
metro_names = plyr::ldply(1:nrow(coords), function(i) {
  add = ggmap::revgeocode(coords[i, ], output = "more")
  x = 2
  while (is.na(add$address) & x > 0) {
    add = ggmap::revgeocode(coords[i, ], output = "more")
    # just try three times
    x = x - 1
  }
  # give the server a bit time
  Sys.sleep(sample(seq(1, 4, 0.1), 1))
  # return the result
  add
})
```

Choosing `more` as `revgeocode()`'s `output` option will give back a `data.frame` with several columns referring to the location including the address, locality and various administrative levels.
Overall, we are satisfied with the `locality` column serving as metropolitan names (Munich, Nuremberg, Stuttgart, Frankfurt, Hamburg, Berlin, Leipzig) bar one exception, namely Velbert.
Hence, we replace Velbert with the corresponding name in the `administrative_area_level_2` column, that is Düsseldorf (Fig. \@ref(fig:metro-areas)).

```{r, eval = FALSE}
metro_names = select(locality, administrative_area_level_2)
# replace Velbert
metro_names = 
  mutate(metro_names, 
         locality = ifelse(locality == "Velbert",
                           administrative_area_level_2, locality)) %>%
  select(locality) %>%
  pull
```

## Points of interests

The **osmdata** package provides a fantastic and easy-to-use interface to download OSM data (see also section \@ref(retrieving-data)).
Instead of downloading all shops for the whole of Germany, we restrict the download to the defined metropolitan areas. 
This relieves the OSM server resources, reduces download time and above all only gives back the shop locations we are interested in.
The `lapply()` loop runs through all eight metropolitan names which subsequently define the bounding box in the `opq()` function (see section \@ref(retrieving-data)).
Alternatively, we could have provided the bounding box in the form of coordinates ourselves.
Next, we indicate that we only would like to download `shop` features (see this [page](http://wiki.openstreetmap.org/wiki/Map_Features) for a full list of OpenStreetMap map features).
`osmdata_sf()` returns a list with several spatial objects (points, lines, polygons, etc.).
Here, we will only keep the point objects.
As with Google's reverse geocode API, the OSM-download will once in a while not work at the first attempt.
The `while`-loop increases the number of download trials to three. 
If then still no features can be downloaded, most likely there are none.
However, is is highly unlikely that there are no shops in any of our defined metropolitan areas.
The `if`-condition at the end of the following code chunk checks if there is at least one shop for each region.

```{r, eval = FALSE}
library(osmdata)
shops = lapply(metro_names, function(x) {
  # give the server a bit time
  Sys.sleep(sample(seq(5, 10, 0.1), 1))
  query = opq(x) %>%
    add_osm_feature(query, key = "shop")
  points = osmdata_sf(query)
  # request the same data again if nothing has been downloaded
  iter = 2
  while (nrow(points$osm_points) == 0 & iter > 0) {
    points = osmdata_sf(query)
    iter = iter - 1
  }
  points = points$osm_points
  points = st_set_crs(points, 4326)
  })
# checking if we have downloaded shops for each metropolitan area
if (any(sapply(shops, nrow) == 0)) {
  message("There is still a metropolitan area without any features. Please fix!")
}
```

To make sure that each list object comes with the same column, we only keep the `shop` and the `id` columns with the help of another `lapply`-loop.
This is not a given since OSM contributors are not equally meticulous when collecting data.
Finally, we `rbind` all shops into one large `sf`-object.
<!-- maybe you should also keep the ID column and only keep unique elements -->

```{r, eval = FALSE}
# select only the shop column and rbind all list elements
shops = lapply(shops, select, shop)
shops = Reduce(rbind, shops)
```

The only thing left to do is to convert the spatial point object into a raster.
The `rasterize()` function does exactly this.
As input it expects an `sp` vector object, here we use the spatial point object `shops`, and a raster object, here we use the population raster whose original values have been replaced by `NA`s.
A function defines how the values from the spatial vector object are transferred to the raster object. 
`count` simply counts the number of spatial object, here the shop points, falling into one raster cell, and returns this value as the output cell value.
Hence, we end up with a shop density, namely the number of shops per square kilometer.
Naturally, the projection of the input raster and the input vector object have to match which is why we first have to reproject our shops to the CRS of the raster object before rasterizing.

```{r, eval = FALSE}
shops$id = 1
shops = st_transform(shops, proj4string(input$pop))
shops = as(shops, "Spatial")
# create poi raster
poi = input$pop
poi[] = NA
poi = rasterize(x = shops, y = poi, field = "id", fun = "count")
# rasterize(shops, poi_2, field = "shop", fun = function(x, ...) length(x))
# poi = rasterize(shops, poi_2, field = "count", fun = "count")
# gives another result because there are NAs in the shop column
# poi = rasterize(shops, poi_2, field = "shop", fun = "count")
plot(poi, xlim = c(4380000, 4420000), ylim = c(2920000, 2940000))
```

```{r, eval = FALSE}
library(classInt)
# construct reclassification matrix
int = classInt::classIntervals(values(poi), n = 5, style = "fisher")
int = round(int$brks)
rcl_poi = matrix(c(int[1], rep(int[-c(1, length(int))], each = 2), 
                   int[length(int)] + 1), ncol = 2, byrow = TRUE)
rcl_poi = cbind(rcl_poi, 1:5)  
# reclassify
poi = reclassify(poi, rcl = rcl_poi, right = NA) 
names(poi) = "poi"
```

## Finding suitable locations

```{r, eval = FALSE}
# dismiss population raster
reclass = dropLayer(reclass, "pop")
# add poi raster
reclass = addLayer(reclass, poi)
# calculate the total score
result = sum(reclass)
# have a look at suitable bike shop locations in Berlin
plot(result > 10, xlim = c(4540000, 4560000), ylim = c(3260000, 3290000))
mapview(result > 10, xlim = c(4540000, 4560000), ylim = c(3260000, 3290000))
```
zoom into Berlin 
mapview(berlin) + mapview(suitable)

## Discussion and next steps
Here, normative Nutzung von GIS (frequently termed expert-based and provided as justification in the absence of better data) which is typical for (geo-)marketing applications.

classification of rasters, weight of rasters

population raster 100 m much more detailed
INSPIRE database -> much more data available at a greater level of detail
Interactions remained unconsidered. For example, men and single households?


next steps:
So far we have merely identified areas of possible interest for a bike shop in accordance with our survey.
In an advanced chapter we could continue the analysis as follows:
 - restrict our analysis to Nuremberg (but point out that we could do the whole analysis simultaneously for all metropolitan areas in Germany)
 - choose optimal location based on number of inhabitants -> you should reach as much people within 15 minutes as possible (routing, catchment area)
 - the farther away we get from a shop, the more unlikely people will go to our shop (-> decay distance)
 - however, we have to take into account that there are already competitors (huff model, attraction)
 - what about rent prices/housing costs

Reference (Huff model): https://journal.r-project.org/archive/2017/RJ-2017-020/RJ-2017-020.pdf

## Exercises

1. Download the csv file containing inhabitant information for a 100 m cell resolution ( https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip?__blob=publicationFile&v=3).
Please note that the unzipped file has a size of 1.23 GB.
To read it into R you can use `readr::read_csv`.
This takes 30 seconds on my machine (16 GB RAM)
`data.table::fread()` might be even faster, and returns an object of class `data.table()`.
Use `as.tibble()` to convert it into a tibble.
Build an inhabitant raster, aggregate it to a cell resolution of 1 km, and compare the difference with the inhabitant raster (`inh`) we have created using class mean values.
1. In the text we have deleted one polygon of the `metros` object (polygon number 5) since it only touches the border of another polygon.
Recreate the `metros` object and instead of deleting polygon number 5, make it part of the Cologne/Düsseldorf metropolitan region (hint: create a column named region_id, add polygon number 5 to the Cologne/Düsseldorf area and dissolve).
1. Suppose our bike shop predominantly sold E-bikes to older people. 
Change the age raster accordingly, repeat the remaining analyses and compare the changes with our original result.
