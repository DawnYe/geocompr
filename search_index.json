[
["index.html", "Geocomputation with R Welcome Development How to contribute? Reproducibility", " Geocomputation with R Robin Lovelace, Jakub Nowosad, Jannes Muenchow 2017-11-12 Welcome Welcome to the online home of Geocomputation with R, a forthcoming book with CRC Press. Development Inspired by bookdown and other open source projects we are developing this book in the open. Why? To encourage contributions, ensure reproducibility and provide access to the material as it evolves. The book’s development can be divided into four main phases: Foundations Basic applications Geocomputation methods Advanced applications Currently the focus is on Part 2, which we aim to be complete by December. New chapters will be added to this website as the project progresses, hosted at robinlovelace.net/geocompr and kept up-to-date thanks to Travis. Currently the build is: The version of the book you are reading now was built on 2017-11-12 and was built on Travis. How to contribute? bookdown makes editing a book as easy as editing a wiki, provided you have a GitHub account (sign-up at github.com). Once logged-in to GitHub, clicking on the ‘edit me’ icon highlighted in the image below will take you to the source R Markdown where you can make changes: To raise an issue about the book’s content (e.g. code not running) or make a feature request, check-out the issue tracker. Reproducibility To reproduce the book, you need a recent version of R and up-to-date packages, which can be installed with the following command (which requires devtools): devtools::install_github(&quot;robinlovelace/geocompr&quot;) To build the book locally, clone or download the geocompr repo, load R in root directory (e.g. by opening geocompr.Rproj in RStudio) and run the following lines: bookdown::render_book(&quot;index.Rmd&quot;) # to build the book browseURL(&quot;_book/index.html&quot;) # to view it Further details can be found at Robinlovelace/geocompr. "],
["preface.html", "Preface", " Preface This book is aimed at people who want to do spatial data analysis, visualization and modeling with a modern programming language (R). There is no single target audience but we expect the book to be especially useful for: People who have learned spatial analysis skills using a desktop Geographic Information System (GIS) such as QGIS, ArcMap, GRASS or SAGA, who want access to a powerful (geo)statistical and visualization programming language and the benefits of a command-line approach (Sherman 2008): With the advent of ‘modern’ GIS software, most people want to point and click their way through life. That’s good, but there is a tremendous amount of flexibility and power waiting for you with the command line. Graduate students and researchers from fields specializing in geographic data including Geography, Remote Sensing, Planning, GIS and Geographic Data Science Academics and post-graduate students working on projects in fields including Geology, Regional Science, Biology and Ecology, Agricultural Sciences (precision farming), Archaeology, Epidemiology, Transport Modeling, and broadly defined Data Science which require the power and flexibility of R for their research Applied researchers and analysts in public, private or third-sector organisations who need the reproducibility, speed and flexibility of a command-line language such as R in applications dealing with spatial data as diverse as Urban and Transport Planning, Logistics, Geo-marketing (store location analysis) and Emergency Planning The book is designed to be accessible to a wide audience. The chapters progress gradually in difficulty and exercises are provided at the end of each chapter to ensure understanding. However, R novices may find the reproducible code chunks — an integral part of the book — difficult to understand and implement at first. Impatient readers are welcome to dive straight into the first practical examples in Chapter 2. However we suggest most R beginners get up-to-speed with R before diving-in to chapters onwards (unless you’re reading the book for an understanding of the concepts). R has a supportive community that has developed a wealth of resources that can help here, three of which we highly recommend: An introduction to R (Venables, Smith, and Team 2017), R for Data Science (Grolemund and Wickham 2016) and Efficient R Programming (Gillespie and Lovelace 2016), especially Chapter 2 (on installing and setting-up R/RStudio) and Chapter 10 (on learning to learn). There are also many interactive resources, including DataCamp’s Introduction to R and tutorials created with learnr. With such material to hand we are confident that the command-line approach demonstrated in this book will be worthwhile within a few months for most people, including programming novices. Over time and with practice R will likely become the natural choice for many geographic applications. We expect you will eventually find its interface faster than the point-and-click interface emphasized in many desktop GISs. For some applications such as Spatial Statistics and modelling R may be the only realistic way to get the work done. As outlined in section 1.2 there are many reasons for using R for geocomputation: R is well-suited to the interactive use required in many geographic data analysis workflows compared with other languages. R excels in the rapidly growing fields of Data Science (which includes data carpentry, statistical learning techniques and data visualization) and (via efficient interfaces to databases and distributed computing systems) Big Data. For some applications another language may be more appropriate, such as Python, Java or C++. There are excellent resources for learning geocomputation without R, as discussed in section 1.3. However we advocate learning one language (R) in depth to gain an understanding of the concepts. Proficiency with one programming language is preferable to the breadth of knowledge (and potential for confusion) attainable by dabbling with many. We believe that Geocomputation with R will equip you with the knowledge and skills needed to solve the majority of challenges that geographic data pose. A logical question to ask if you’ve read this far may be: what is geocomputation anyway? The answer is provided in section 1.1 which, alongside the rest of Chapter 1, explains the motivations behind this book. References "],
["intro.html", "1 Introduction 1.1 What is geocomputation? 1.2 Why Geocomputation with R? 1.3 Software for geocomputation 1.4 R’s spatial ecosystem 1.5 The history of R-spatial 1.6 Exercises", " 1 Introduction This book is about harnessing the power of modern computers to do things with geographic data. It teaches a range of spatial skills, including: reading, writing and manipulating geographic data; making static and interactive maps; applying geocomputation to solve real-world problems; and modeling geographic phenomena. By demonstrating how various spatial operations can be linked, in reproducible ‘code chunks’ that intersperse the prose, the book also teaches a transparent and thus scientific workflow. Learning how to use the wealth of geospatial tools available from the R command line can be exciting but creating new ones can be truly liberating, by removing constraints on your creativity imposed by software. By the end of the book you should be able to create new tools for geocomputation in the form of shareable R scripts and functions. Over the last few decades free and open source software for geospatial data (‘FOSS4G’) has progressed at an astonishing rate (see foss4g.org). Thanks to FOSS4G and the wider open source movement geospatial analysis is no longer the preserve of those with expensive hardware and software: anyone can now download high performance spatial libraries on their computer. However, despite the growth of geospatial software that is open source, much of it is still not easy to script. Open source Geographic Information Systems (GIS) such as QGIS (see qgis.org) have greatly reduced the ‘barrier to entry’ have an emphasis on the Graphical User Interface (GUI) rather than the Command-Line Interface (CLI). This GUI-centred approach can discourage reproducibility. This book focusses exclusively on the CLI, enabling reproducible, and ‘computational’ workflows, something we will expand on in Chapter 13. Reproducibility is a major advantage of command-line interfaces, but what does it mean in practice? We define it as follows: A process is reproducible only if the same results can be generated by others using publicly accessible code. This may sound simple and easy to acheive (which it is if you carefully maintain your R code in script files) but has profound implications for teaching and the scientific process (Pebesma, Nüst, and Bivand 2012). A major aim of this book is to make geographic data analysis more accessible as part of a reproducible workflow. R is a flexible language that allows access to many spatial software libraries (see section 1.2). Before going into the details of the software, however, it is worth taking a step back and thinking about what we mean by geocomputation. 1.1 What is geocomputation? Geocomputation is a relatively young field with a ~30 year history, dating back to the first conference on the subject in 1996.1 What distinguishes geocomputation from the older quantitative geography, is its emphasis on “creative and experimental” GIS applications (Longley et al. 1998). Additionally, it is also about developing new, research-driven methods (Openshaw and Abrahart 2000): GeoComputation is about using the various different types of geodata and about developing relevant geo-tools within the overall context of a ‘scientific’ approach. But geocomputation and this book teach more than just methods and code: they are about doing “practical work that is beneficial or useful” (Openshaw and Abrahart 2000). Of course, reading this book will give you a solid knowledge of geocomputational methods, and how to use them via the reproducible examples implemented in the code chunks in each chapter. But there is much more. This book aims to teach how to do geocomputation rather than just to think about it. Hence, you should be also able to apply the learned methods and mastered skills to real-world data, for evidence-based analysis in your own areas of interest. Moreover, throughout the book we encourage you to make geographic research more reproducible, scientific and socially beneficial. This book is also related to a movement that has been labelled Geographical Information Science (GDS). This recent concept essentially combines ‘data science’ with GIS and, like Geocomputation, can be defined in comparison with GIS (see Table 1.1). The focus on reproducibility and a command-line interface in this book is aligned with GDS. Table 1.1: Differences in emphasis between the fields of Geographic Information Systems (GIS) and Geographic Data Science (GDS). Attribute GIS GDS Home disciplines Geography Geography, Computing, Statistics Software focus Graphical User Interface Code Reproduciblility Minimal Maximal While embracing recent developments in the field, we also wanted to pay respects to the wider field of Geography, with its 2000 history (Roller 2010), and the narrower field of Geographic Information System (GIS) (Neteler and Mitasova 2008). Geography has played an important role in explaining and influencing humanity’s relationship with the natural world2 and this book aims to contribute to this so-called ‘Geographic Tradition’ (Livingstone 1992). GIS has become almost synonymous with handling spatial data on a computer, and provides a basis for excellent open source tools which can be accessed from R, as we will see in Chapter 13. The book’s links to older disciplines were reflected in suggested titles for the book: Geography with R and R for GIS. Each has advantages. The former conveys the message that it comprises much more than just spatial data: non-spatial attribute data are inevitably interwoven with geometry data, and Geography is about more than where something is on the map. The latter communicates that this is a book about using R as a GIS, to perform spatial operations on geographic data (Bivand, Pebesma, and Gómez-Rubio 2013). However, the term GIS conveys some connotations (see Table 1.1) which simply fail to communicate one of R’s greatest strengths: its console-based ability to seamlessly switch between geographic and non-geographic data processing, modeling and visualization tasks. By contrast, the term geocomputation implies reproducible and creative programming. Of course, (geocomputational) algorithms are powerful tools that can become highly complex. However, all algorithms are composed of smaller parts. By teaching you its foundations and underlying structure, we aim to empower you to create your own innovative solutions to geographic data problems. 1.2 Why Geocomputation with R? Early geographers used a variety of tools including rulers, compasses and sextants to advance knowledge about the world. However, until John Harrison invented the marine chronometer in the 18th century it had been impossible to determine the exact longitude at sea (‘the longitude problem’). Prior to his invention ships followed for centuries a line of constant latitude making each journey much longer, more expensive and often also more dangerous. Nowadays this seems unimaginable with every smartphone having a GPS receiver at its disposal. And there are a multitude of other sensors measuring the world in real-time (satellites, radar, autonomous cars, citizens, etc.). For instance, an autonomous car could create 100 GB or more per day (see e.g., this article). Equally, earth observation data (satellite imagery) has become so big that it is impossible to analyze the corresponding data with a single computer (see http://r-spatial.org/2016/11/29/openeo.html). Hence, we need computational power, software and related tools to handle and extract the most interesting patterns of this ever-increasing amount of (geo-)data. (Geo-)Databases help with data management, storing and querying such large amounts of data. Through interfaces we can access subsets of these data for further analysis, information extraction and visualization. In this book we treat R as a ‘tool for the trade’ for the latter. R is a multi-platform, open source language and environment for statistical computing and graphics (https://www.r-project.org/). With a wide range of packages R also supports advanced geospatial statistics, modeling and visualization.3. At its core R is an object-oriented, functional programming language (Wickham 2014), and was specifically designed as an interactive interface to other software (Chambers 2016). The latter also includes many ‘bridges’ to a treasure trove of GIS software, geolibraries and functions. It is thus ideal for quickly creating ‘geo-tools’, without needing to master lower level languages (compared to R) such as C, FORTRAN and Java (see section 1.3). This can feel like breaking free from the metaphorical ‘glass ceiling’ imposed by GUI-based proprietary geographic information systems (see Table 1.1 for a definition of GUI). What is more, advanced users might even extend R with the power of other languages (e.g., C++ through Rcpp or Python through reticulate; see also section 1.3). An example showing R’s flexibility with regard to geographic software development is its support for generating interactive maps thanks to leaflet (Cheng, Karambelkar, and Xie 2017). The packages tmap and mapview (Tennekes 2017; Appelhans et al. 2017) are built on and extend leaflet. These packages help overcome the criticism that R has “limited interactive [plotting] facilities” (Bivand, Pebesma, and Gómez-Rubio 2013). The code below illustrates this by generating Figure 1.1. library(leaflet) popup = c(&quot;Robin&quot;, &quot;Jakub&quot;, &quot;Jannes&quot;) leaflet() %&gt;% addProviderTiles(&quot;NASAGIBS.ViirsEarthAtNight2012&quot;) %&gt;% addAwesomeMarkers(lng = c(-3, 23, 11), lat = c(52, 53, 49), popup = popup) Figure 1.1: World at night imagery from NASA overlaid by the authors’ approximate home locations to illustrate interactive mapping with R. It would have been difficult to produce Figure 1.1 using R a few years ago, let alone embed the results in an interactive html page (the interactive version can be viewed at robinlovelace.net/geocompr). This illustrates R’s flexibility and how, thanks to developments such as knitr and leaflet, it can be used as an interface to other software, a theme that will recur throughout this book. The use of R code, therefore, enables teaching geocomputation with reference to reproducible examples such as that provided in 1.1 rather than abstract concepts. 1.3 Software for geocomputation R is a powerful language for geocomptation but there are many other options for spatial data analysis. Awareness of these will help situate R in the wider geospatial ecosystem, and identify when a different tool may be more appropriate for a specific task. Over time R developers have added various R interfaces (or ‘bridges’ as we call them in Chapter 13) to other software. Therefore knowing what else is out there can also be useful from an R-spatial perspective because a) there may already be a bridge to something that adds the functionality you need and b) if there’s not already a bridge there may be on the horizon. With this motivation in mind the section briefly introduces the languages C++, Java and Python for geocomputation, with reference to R. A natural choice for geocomputation would be C++ since major GIS packages (e.g., GDAL, QGIS, GRASS, SAGA, and even ArcGIS) often rely in great parts on it. This is because well-written C++ can be blazingly fast, which makes it a good choice for performance-critical applications such as the processing of large spatial data. Usually, people find it harder to learn than Python or R. It is also likely that you have to invest a lot of time to code things that are readily available in R. Therefore, we would recommend to learn R, and subsequently to learn C++ through Rcpp if a need for performance optimization arises. Subsequently, you could even implement geoalgorithms you are missing from the most common desktop GIS with the help of Rcpp4. Java is another important (and versatile) language used in GIScience. For example, the open-source desktop GIS gvSig, OpenJump and uDig are written in Java. There are also many open source add-on libraries available for Java, including GeoTools and the Java Topology Suite.5 Furthermore, many server-based applications use Java including among others Geoserver/Geonode, deegree and 52°North WPS. Java’s object-oriented syntax is similar to that of C++ but its memory management, at least from a user’s perspective, is simpler and more robust. Java is rather unforgiving regarding class, object and variable declarations, which encourages well-designed programming structure, useful in large projects with thousands of lines of codes placed in numerous files. Following the write once, run anywhere principle, Java is platform-independent (which is unusual for a compiled language) and has excellent performance on large-scale systems. This makes Java a suitable language for complex architecture projects such RStudio, the Integrated Development Environment (IDE) in which this book was written! Java is less suitable for statistical modeling and visualization than Python or R. Although Java can be used for data science (Brzustowicz 2017), it has relatively few statistical libraries, especially compared with R. Furthermore Java is hard to use interactively. Interpreted languages (such as R and Python) are better suited for the type of interactive workflow used in many geographic workflows than compiled languages (such as Java and C++). Unlike Java (and most other languages) R has native support for data frames and matrices, making it especially well suited for (geographic) data analysis. Python is the final language for geocomputation that deserves attention in this section. Like R, Python has gained popularity due to the rapid growth of data science (Robinson 2017). Both languages are object-oriented, and have lots of further things in common. Due to their similarities there is much on-line discussion framing the relative merits of each language as a competition, as exemplified by an infographic by DataCamp titled “DATA SCIENCE WARS: R vs Python”, which arguably generates more heat than light. In practice both languages have their strengths and to some extent which you use is less important than domain of application and the communication of results. Learning either will provide a head-start in learning the other. However there are major advantages of R over Python for geocomputation which explains its prominence in this book. R has unparalled support for statistics, including spatial statistics, with hundreds of packages (unmatched by Python) supporting thousands of statistical methods. The major advantage of Python is that it is a general-purpose programming language. It is well-suited to many applications, including desktop software, computer games, websites and data science. R, by contrast, is primarily suited to the latter, under a broad definition of data science.6 This also explains Python’s larger user base compared with R’s. Python is often the only shared language between different (geocomputation) communities, explaining why it has become the ‘glue’ that holds many GIS programs together. Many geoalgorithms, including those in QGIS and ArcMap, can be accessed from the Python command line, making it well-suited as a starter language for command-line GIS.7 For spatial statistics and predictive modeling, however, R is second-to-none. This does not mean you must chose either R or Python: Python supports most common statistical techniques (though R tends to support new developments in spatial statistics earlier) and many concepts learned from Python can be applied to the R world. Like R Python also supports spatial data analysis and manipulation with packages such as osgeo, Shapely, NumPy and PyGeoProcessing (Garrard 2016). 1.4 R’s spatial ecosystem Before cracking-on with the action, a few introductory remarks are needed to explain the approach taken here and provide context. There are many ways to handle spatial data in R, with dozens of packages in the area.8 In this book we endeavor to teach the state-of-the-art in the field whilst ensuring that the methods are future-proof. Like many areas of software development, R’s spatial ecosystem is rapidly evolving. Because R is open source, these developments can easily build on previous work, by ‘standing on the shoulders of giants’, as Isaac Newton put it in 1675. This approach is advantageous because it encourages collaboration and avoids ‘reinventing the wheel’. The package sf (covered in Chapter 2), for example, builds on its predecessor sp. A surge in development time (and interest) in ‘R-Geo’ has followed the award of a grant by the R Consortium for the development of support for Simple Features, an open-source standard and model to store and access vector geometries. This resulted in the sf package (covered in 2.1.1). Multiple places reflect the immense interest in sf. This is especially true for the R-sig-Geo Archives, a long-standing open access email list containing much R-spatial wisdom accumulated over the years. Many posts on the list now discuss sf and related packages, suggesting that R’s spatial software developers are using the package and, therefore, it is here to stay. We even propose that the release of sf heralds a new era for spatial data analysis and geocomputation in R. This era9 clearly has the wind in its sails, and is set to dominate future developments in R’s spatial ecosystem for years to come. So time invested in learning the ‘new ways’ of handling spatial data and, hopefully, reading this book, is well spent! Figure 1.2: The popularity of spatial packages in R. The y-axis shows the average number of downloads, within a 30-day rolling window, of R’s top 5 spatial packages, defined as those with the highest number of downloads within the last 30 days. It is noteworthy that shifts in the wider R community, as exemplified by the data processing package dplyr (released in 2014) influenced shifts in R’s spatial ecosystem. Alongside other packages that have a shared style and emphasis on ‘tidy data’ (including e.g., ggplot2), dplyr was placed in the tidyverse ‘metapackage’ in late 2016. The tidyverse approach, with its focus on long-form data and fast, intuitively named functions, has become immensely popular. This has led to a demand for ‘tidy spatial data’ which has been partly met by sf and other approaches such as tabularaster. An obvious feature of the tidyverse is the tendency for packages to work in harmony. Although an equivalent geoverse is presently missing, there is an on-going discussion of harmonizing R’s many spatial packages10 and a growing number of actively developed packages which are designed to work in harmony with sf (Table 1.2). Table 1.2: The top 5 most downloaded packages that depend on sf, in terms of average number of downloads per day over the previous month. As of 2017-11-04 there are 35 packages which import sf. package Downloads plotly 1883 leaflet 630 mapview 234 geojsonio 218 rmapshaper 157 1.5 The history of R-spatial There are many benefits of using recent spatial packages such as sf, as advocated in this book. However it is also important to be aware of the history of R’s spatial capabilities: many functions, use-cases and teaching material are contained in older packages. These can still be useful today, provided you know where to look. R’s spatial capabilities originated in early spatial packages in the S language (Bivand and Gebhardt 2000). The 1990s saw the development of numerous S scripts and a handful of packages for spatial statistics. R packages arose from these and by 2000 there were R packages for various spatial methods “point pattern analysis, geostatistics, exploratory spatial data analysis and spatial econometrics”, according to an article presented at GeoComputation2000 (Bivand and Neteler 2000) Some of these, notably spatial, sgeostat and splancs are still available on CRAN (Rowlingson and Diggle 1993; Rowlingson and Diggle 2017; Venables and Ripley 2002; University and Gebhardt 2016). A subsequent article in R News (the predecessor of The R Journal) contained an overview of spatial statistical software in R at the time, much of which was based on previous code written for S/S-PLUS (Ripley 2001). This overview described packages for spatial smoothing and interpolation (e.g., akima, spatial, sgeostat and geoR) and point pattern analysis (splancs and spatstat; Akima and Gebhardt 2016; Rowlingson and Diggle 2017; Jr and Diggle 2016). While all these are still available on CRAN, spatstat stands out among them, as it remains dominant in the field of spatial point pattern analysis (Baddeley, Rubak, and Turner 2015). The following R News issue (Volume 1/3) put spatial packages in the spotlight again, with an introduction to splancs and a commentary on future prospects regarding spatial statistics (Bivand 2001). Additionally, the issue introduced two packages for testing spatial autocorrelation that eventually became part of spdep (Bivand 2017). Notably, the commentary mentions the need for standardization of spatial interfaces, efficient mechanisms for exchanging data with GIS, and handling of spatial metadata such as coordinate reference systems (CRS). maptools (written by Nicholas Lewin-Koh; Bivand and Lewin-Koh 2017) is another important package from this time. Initially, maptools just contained a wrapper around shapelib, and permitted the reading of ESRI Shapefiles into geometry nested lists. The corresponding and nowadays obsolete S3 class called “Map” stored this list alongside an attribute data frame. The work on the “Map” class representation was nevertheless important since it directly fed into sp prior to its publication on CRAN. In 2003, Bivand (2003) published an extended review of spatial packages. Around this time the development of R’s spatial capabilities increasingly supported interfaces to external libraries, especially to GDAL and PROJ.4. These interfaces facilitated geographic data I/O (covered in chapter 5) and CRS transformations, respectively. Bivand (2003) proposed a spatial data class system, including support for points, lines, polygons and grids based on GDAL’s support for a wide range of spatial data formats. All these ideas contributed to the packages rgdal and sp, which became the foundational packages for spatial data analysis with R (Bivand, Pebesma, and Gómez-Rubio 2013). rgdal provided GDAL bindings for R which greatly extended R’s spatial capabilities in terms of access to previously unavailable spatial data formats. Initially, Tim Keitt released it in 2003 with support for raster drivers. But soon, rgdal also enabled storing information about coordinate reference system (building on top of the PROJ.4 library), allowed map projections, datum transformations and OGR vector reading. Many of these additional capabilities were thanks to Barry Rowlingson who folded them into the rgdal codebase in March 2006.11 sp, released in 2005, overcame R’s inability to distinguish spatial and non-spatial objects (Pebesma and Bivand 2005). It grew from a workshop before, and a session at the 2003 DSC conference in Vienna, gathering input from most interested package developers. At the same time, sourceforge was chosen for development collaboration (migrated to R-Forge five years later) and the R-sig-geo mailing list was started. Prior to 2005, spatial coordinates were generally treated as any other number. This changed with sp as it provided generic classes and methods for spatial data. The sophisticated class system supported points, lines, polygons and grids, with and without attribute data. Making use of the S4 class system, sp stores each piece of ‘spatially specific’ information (such as bounding box, coordinate reference system, attribute table) in slots, which are accessible via the @ symbol. For instance, sp-classes store attribute data in the data slot as a data.frame. This enables non-spatial data operations to work alongside spatial operations (see section 2.1.2). Additionally, sp implemented generic methods for spatial data types for well-known functions such as summary() and plot() . In the following, sp classes rapidly became the go-to standard for spatial data in R, resulting in a proliferation of packages that depended on it from around 20 in 2008 and over 100 in 2013 (Bivand, Pebesma, and Gómez-Rubio 2013). Now more than 450 packages rely on sp, making it an important part of the R ecosystem. Prominent R packages using sp include: gstat, for spatial and spatio-temporal geostatistics; geosphere, for spherical trigonometry; and adehabitat used for the analysis of habitat selection by animals (Pebesma and Graeler 2017; Calenge 2006; Hijmans 2016a). While rgdal and sp solved many spatial issues, R was still lacking geometry calculation abilities. Therefore, Colin Rundel started to develop a package that interfaces GEOS, an open-source geometry library, during a Google Summer of Coding project in 2010. The resulting rgeos package (first released in 2010; Bivand and Rundel 2017) brought geometry calculations to R by allowing functions and operators from the GEOS library to manipulate sp objects. Another limitation of sp was its limited support of raster data. The raster-package (first released in 2010; Hijmans 2016b) overcame this by providing Raster* classes and functions for creating, reading and writing raster data. A key feature of raster is its ability to work with datasets that are too large to fit into the main memory (RAM), thereby overcoming one of R’s major limitations when working on large raster data.12 In parallel with or partly even preceding the development of spatial classes and methods came the support for R as an interface to dedicated GIS software. The GRASS package (Bivand 2000) and follow-on packages spgrass6 and rgrass7 (for GRASS GIS 6 and 7, respectively) were prominent examples in this direction (Bivand 2016b; Bivand 2016a). Other examples of bridges between R and GIS include RSAGA (Brenning and Bangs 2016, first published in 2008), ArcGIS (Brenning 2012, first published in 2008), and RQGIS (Muenchow and Schratz 2017, first published in 2016). Map making was not a focus of R’s early spatial capabilities. But soon sp provided methods for advanced map making using both the base and lattice plotting system. Despite this, a demand for the layered grammar of graphics was growing especially after the release of ggplot2 in 2007. ggmap extended ggplot2’s spatial capabilities (Kahle and Wickham 2013). However, its main purpose was the easy access of several APIs to automatically download map tiles (among others, Google Maps and OpenStreetmap) and subsequent plotting of these as a basemap. Though ggmap facilitated map-making with ggplot2, one main limitation remained. To make spatial data work with the ggplot2 system, one needed to fortify spatial objects. Basically, this means, you need to combine the coordinates and attribute slots of a spatial class object into one data frame. While this works well in the case of points, it duplicates the same information over and over again in the case of polygons, since each coordinate (vertex) of a polygon receives the attribute data of the polygon. This is especially disadvantageous if you need to deal with tens of thousands of polygons. With the introduction of simple features to R this limitation disappears, and it seems likely that this will make ggplot2 the standard tool for the visualization of vector data. This might be different regarding the visualization of raster data. Raster visualization methods received a boost with the release of rasterVis (Lamigueiro 2014) which builds on top of the lattice system. More recently, new packages aim at easing the creation of complex, high-quality maps with minimal code. The tmap package (released in 2014) might serve as an archetype for this kind of development (Tennekes 2017). It facilitates the user-friendly creation of thematic maps with an intuitive command-line interface (see also mapmisc) . tmap is a sophisticated yet user friendly mapping package which works in harmony with the leaflet package (released in 2015) for interactive map making (Cheng, Karambelkar, and Xie 2017). Similarly, the mapview package builds also on top of leaflet (Appelhans et al. 2017) for interactive mapping based on sp or sf objects. mapview allows the access of a wide range of background maps, scale bars and more. However, it is noteworthy that among all the recent developments described above, the support for simple features (sf; Pebesma 2017) has been without doubt the most important evolution in R’s spatial ecosystem. Naturally, this is the reason why we will describe sf in detail in Chapter 2. 1.6 Exercises Think about the terms ‘GIS’, ‘GDS’ and ‘Geocomputation’ described above. Which is your favorite, and why? Provide three reasons for using a scriptable language such as R for geocomputation instead of using an established GIS program such as QGIS. Name two advantages and two disadvantages of using mature packages compared with ‘cutting edge’ packages for spatial data (for example sp vs sf). References "],
["spatial-class.html", "2 Geographic data in R Prerequisites 2.1 Vector data 2.2 Raster data 2.3 Coordinate Reference Systems 2.4 Units 2.5 Exercises", " 2 Geographic data in R Prerequisites This is the first practical chapter of the book, and therefore it comes with some software requirements. We assume that you have installed on your computer a recent version of R and that you are comfortable using it at the command line, e.g., via an integrated development environment (IDE) such as RStudio (recommended). R/RStudio works on all major operating systems. You can install and set up both in a few minutes on most modern computers, as described in section 2.3 and section 2.5 of Gillespie and Lovelace (2016). If you are not a regular R user, we recommend that you familiarize yourself with the language before proceeding with this chapter. You can do so using resources such as Gillespie and Lovelace (2016), Grolemund and Wickham (2016) as well as online interactive guides such as DataCamp. We recommend organising your work as you learn, for example with the help of an RStudio ‘project’ called geocomp-learning or similar and a new script for each chapter. The code you type to help learn the content of this chapter, for example, could be placed in a script called chapter-02.R. Everyone learns in a different way so it is important that you structure your code in a way that makes sense to you and that you avoid copy-pasting to get used to typing code. After R/RStudio are installed and up-to-date — use update.packages() to update packages — the next step is to install and load the packages used in this chapter. This can be done with install.packages(&quot;package_name&quot;) (not shown) and library(package_name) functions: library(raster) # classes and functions for raster data library(sf) # classes and functions for vector data The chapter also relies on two data packages: spData and spDataLarge, the latter of which must be installed after spData is loaded:13 library(spData) # load geographic data install.packages(&quot;spDataLarge&quot;) # after loading spData library(spDataLarge) # load geographic data On Mac and Linux a few requirements must be met to install sf. These are described in the package’s README at github.com/r-spatial/sf. This chapter will provide brief explanations of the fundamental geographic data models: vector and raster. We will introduce the theory behind each data model and the disciplines in which they predominate, before demonstrating their implementation in R. The vector data model represents the world using points, lines and polygons. This means, it supports data with discrete, well-defined borders. Generally, vector datasets have a high level of precision (but not necessarily accuracy as we will see in 2.4). The raster data model is good at representing continuous phenomena such as elevation or rainfall with the help of grid cells that divide the surface up into a cells of constant size (resolution). Rasters aggregate spatially specific features to a given resolution, meaning that they are spatially consistent and scalable (many worldwide raster datasets are available). The downside of this is that small features can be blurred or lost. The appropriate data model to use depends on the domain of application: Vector data tends to dominate the social sciences because human settlements and boundaries have discrete borders. By contrast, raster data often dominates the environmental sciences because these often use remotely sensed imagery. However, there is a substantial level of overlap: ecologists and demographers, for example, commonly use both vector and raster data. We, therefore, strongly recommend learning about each data model before proceeding to understand how to manipulate them in subsequent chapters. This book uses sf and raster packages to work with vector data and raster datasets respectively. 2.1 Vector data Take care when using the word ‘vector’ as it can have two meanings in this book: geographic vector data and vectors (note the monospace font) in R. The former is a data model, the latter is an R class just like data.frame and matrix. Still, there is a link between the two: the spatial coordinates which are at the heart of the geographic vector data model can be represented in R using vector objects. The geographic vector model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g. the location of a bus stop) or they can be linked together to form more complex geometries such as lines and polygons. Most point geometries contain only two dimension (3 dimensional CRSs contain an additional \\(z\\) value, typically representing height above sea level). In this system London, for example, can be represented by the coordinates c(-0.1, 51.5). This mean its location is -0.1 degrees east and 55.5 degrees north of the origin at 0 degrees longitude (the Prime Meridian) and 0 degree latitude (the Equator) in a geographic (‘lon/lat’) CRS (Figure 2.1, left panel). The same point could also be approximated in a projected CRS with ‘Easting/Northing’ values of c(530000, 180000) in the British National Grid (BNG). In the vector data model this suggests that London is located 530 km East and 180 km North of the \\(origin\\) of the CRS. This can be verified visually: slightly more than 5 ‘boxes’ — square areas bounded by the grey grid lines 100 km in width — separate the point representing London from the origin in Figure 2.1. This shows that the origin of the BNG is located in the sea to the southeast of the UK, ensuring that all locations in the UK can be represented with positive Easting and Northing values. CRSs are described in section 2.3. Figure 2.1: Illustration of vector (point) data in R with the location of London (the red X) represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° Longitude and latitude. The right plot represents a projected CRS with an origin at -2° longitude and 49° latitude, and units of meters. 2.1.1 An introduction to simple features Simple features is an open standard developed and endorsed by the Open Geospatial Consortium (OGC) to represent a wide range of geographic information. It is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class. Only 7 out of 68 possible types of simple feature are currently used in the vast majority of GIS operations (Figure 2.2). The R package sf (Pebesma 2017) fully supports all of these (including plotting methods etc.).14 Figure 2.2: The subset of the Simple Features class hierarchy supported by sf. sf can represent all common vector geometry types (raster data classes are not supported by sf): points, lines, polygons and their respective ‘multi’ versions (which group together features of the same type into a single feature). sf also supports geometry collections, which can contain multiple geometry types in a single object. Given the breadth of geographic data forms, it may come as a surprise that a class system to support all of them is provided in a single package, which can be installed from CRAN:15 sf incorporates the functionality of the three main packages of the sp paradigm (sp (Pebesma and Bivand 2017) for the class system, rgdal (Bivand, Keitt, and Rowlingson 2017) for reading and writing data, rgeos (Bivand and Rundel 2017) for spatial operations undertaken by GEOS) in a single, cohesive whole. This is well-documented in sf’s vignettes: vignette(&quot;sf1&quot;) # for an introduction to the package vignette(&quot;sf2&quot;) # for reading, writing and converting Simple Features vignette(&quot;sf3&quot;) # for manipulating Simple Features As the first vignette explains, simple feature objects in R are stored in a data frame, with geographic data occupying a special column, a ‘list-column’. This column is usually named ‘geom’ or ‘geometry’. We will use the world dataset provided by the spData, loaded at the beginning of this chapter (see nowosad.github.io/spData for a list datasets loaded by the package). world is a spatial object containing spatial and attribute columns, the names of which are returned by the function names() (the last column contains the geographic information): names(world) #&gt; [1] &quot;iso_a2&quot; &quot;name_long&quot; &quot;continent&quot; &quot;region_un&quot; &quot;subregion&quot; #&gt; [6] &quot;type&quot; &quot;area_km2&quot; &quot;pop&quot; &quot;lifeExp&quot; &quot;gdpPercap&quot; #&gt; [11] &quot;geom&quot; It is the contents of this modest-looking geom column that gives sf objects their spatial powers, a ‘list-column’ that contains all the coordinates. The sf package provides a plot() method for vizualising geographic data: the follow command creates Figure 2.3. plot(world) #&gt; Warning: plotting the first 9 out of 10 attributes; use max.plot = 10 to #&gt; plot all #&gt; Warning in classInt::classIntervals(values, nbreaks, breaks): var has #&gt; missing values, omitted in finding classes #&gt; Warning in classInt::classIntervals(values, nbreaks, breaks): var has #&gt; missing values, omitted in finding classes Figure 2.3: A spatial plot of the world using the sf package, with a facet for each attribute. Note that instead of creating a single map, as most GIS programs would, the plot() command has created multiple maps, one for each variable in the world datasets. This behavior can be useful for exploring the spatial distribution of different variables and is discussed further in 2.1.3 below. Being able to treat spatial objects as regular data frames with spatial powers has many advantages, especially if you are already used to working with data frames. The commonly used summary() function, for example, provides a useful overview of the variables within the world object. summary(world[&quot;lifeExp&quot;]) #&gt; lifeExp geom #&gt; Min. :48.9 MULTIPOLYGON :177 #&gt; 1st Qu.:64.3 epsg:4326 : 0 #&gt; Median :72.8 +proj=long...: 0 #&gt; Mean :70.6 #&gt; 3rd Qu.:77.1 #&gt; Max. :83.6 #&gt; NA&#39;s :9 Although we have only selected one variable for the summary command, it also outputs a report on the geometry. This demonstrates the ‘sticky’ behavior of the geometry columns of sf objects, meaning the geometry is kept unless the user deliberately removes them, as we’ll see in section 3.2. The result provides a quick summary of both the non-spatial and spatial data contained in world: the average life expectancy is 73 years (ranging from less than 50 to more than 80 years) across all countries. The word MULTIPOLYGON in the summary output above refers to the geometry type of features (countries) in the world object. This representation is necessary for countries with islands such as Indonesia and Greece. Other geometry types are described in section 2.1.5. It is worth taking a deeper look at the basic behavior and contents of this simple feature object, which can usefully be thought of as a ’Spatial dataFrame). sf objects are easy to subset. The code below shows its first two rows and three columns. The output shows two major differences compared with a regular data.frame: the inclusion of additional geographic data (geometry type, dimension, bbox and CRS information - epsg (SRID), proj4string), and the presence of final geometry column: world[1:2, 1:3] #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent geom #&gt; 1 AF Afghanistan Asia MULTIPOLYGON (((61.21081709... #&gt; 2 AO Angola Africa MULTIPOLYGON (((16.32652835... All this may seem rather complex, especially for a class system that is supposed to be simple. However, there are good reasons for organizing things this way and using sf. 2.1.2 Why simple features? Simple features is a widely supported data model that underlies data structures in many GIS applications including QGIS and PostGIS. A major advantage of this is that using the data model ensures your work is cross-transferable to other set-ups, for example importing from and exporting to spatial databases. A more specific question from an R perspective is “why use the sf package when sp is already tried and tested”? There are many reasons (linked to the advantages of the simple features model) including: Fast reading and writing of data Enhanced plotting performance sf objects can be treated as data frames in most operations sf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages sf function names are relatively consistent and intuitive (all begin with st_) Due to such advantages some spatial packages (including tmap, mapview and tidycensus) have added support for sf. However, it will take many years for many packages to transition, and some packages will never switch. Fortunately these can still be used in a workflow based on sf objects, by converting them to the Spatial class used in sp: library(sp) world_sp = as(world, Class = &quot;Spatial&quot;) # sp functions ... Spatial objects can be converted back to sf in the same way or with st_as_sf(): world_sf = st_as_sf(world_sp, &quot;sf&quot;) 2.1.3 Basic map making You can quickly create basic maps in sf with the base plot() function. By default, sf creates a multi-panel plot (like sp’s spplot()), one sub-plot for each variable (see left-hand image in Figure 2.4). plot(world[3:4]) plot(world[&quot;pop&quot;]) Figure 2.4: Plotting with sf, with multiple variables (left) and a single variable (right). As with sp, you can add further layers to your maps using the add = TRUE-argument of the plot() function .16 To illustrate this, and prepare for content covered in chapters 3 and 4 on attribute and spatial data operations, we will subset and combine countries in the world object, which creates a single object representing Asia: asia = world[world$continent == &quot;Asia&quot;, ] asia = st_union(asia) We can now plot the Asian continent over a map of the world. Note, however, that this only works if the initial plot has only one layer: plot(world[&quot;pop&quot;]) #&gt; Warning in classInt::classIntervals(values, nbreaks, breaks): var has #&gt; missing values, omitted in finding classes plot(asia, add = TRUE, col = &quot;red&quot;) Figure 2.5: A plot of Asia added as a layer on top of countries worldwide. This can be very useful for quickly checking the geographic correspondence between two or more layers: the plot() function is fast to execute and requires few lines of code, but does not create interactive maps with a wide range of options. For more advanced map making we recommend using a dedicated visualization package such as tmap, ggplot2, mapview, or leaflet. 2.1.4 Base plot arguments sf simplifies spatial data objects compared with sp and provides a near-direct interface to GDAL and GEOS C++ functions. In theory this should make sf faster than sp/rgdal/rgeos, something that is tested in Chapter 5 for data I/O. This section introduces sf classes in preparation for subsequent chapters which deal with vector data (in particular Chapter 4). As a final exercise, we will see one way of how to do a spatial overlay in sf. First, we convert the countries of the world into centroids, and then subset those in Asia. Finally, the summary-command tells us how many centroids (countries) are part of Asia (43) and how many are not (134). world_centroids = st_centroid(world) #&gt; Warning in st_centroid.sfc(st_geometry(x), of_largest_polygon = #&gt; of_largest_polygon): st_centroid does not give correct centroids for #&gt; longitude/latitude data sel_asia = st_intersects(world_centroids, asia, sparse = FALSE) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar summary(sel_asia) #&gt; V1 #&gt; Mode :logical #&gt; FALSE:134 #&gt; TRUE :43 Note: st_intersects() uses GEOS in the background for the spatial overlay operation (see also Chapter 4). Since sf’s plot() function builds on base plotting methods, you may also use its many optional arguments (see ?plot and ?par). This provides a powerful but not necessarily intuitive interface. To make the area of circles proportional to population, for example, the cex argument can be used as follows (see Figure 2.6 created with the code below and the exercises in section 2.5): plot(world[&quot;continent&quot;]) plot(world_centroids, add = TRUE, cex = sqrt(world$pop) / 10000) Figure 2.6: Country continents (represented by fill colour) and 2015 populations (represented by points, with point area proportional to population) worldwide. 2.1.5 Simple feature classes To understand new data formats in depth, it often helps to build them from the ground up. This section walks you through vector spatial classes step-by-step, from the elementary simple feature geometry to simple feature objects of class sf representing complex spatial data. Before describing each geometry type that the sf package supports, it is worth taking a step back to understand the building blocks of sf objects. As stated in section 2.1.1, simple features are simply data frames with at least one special column that makes it spatial. These spatial columns are often called geom or geometry and can be like non-spatial columns: world$geom refers to the spatial element of the world object described above. These geometry columns are ‘list columns’ of class sfc: they are simple feature collections. In turn, sfc objects are composed of one or more objects of class sfg: simple feature geometries. To understand how the spatial components of simple features work, it is vital to understand simple feature geometries. For this reason we cover each currently supported sfg type in the next subsections before moving on to describe how these can be combined to form sfc and eventually full sf objects. 2.1.5.1 Simple feature geometry types Geometries are the basic building blocks of simple features. Simple features in R can take on one of the 17 geometry types supported by the sf package. In this chapter we will focus on the seven most commonly used types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. Find the whole list of possible feature types in the PostGIS manual. Generally, well-known binary (WKB) or well-known text (WKT) are the standard encoding for simple feature geometries. WKB representations are usually hexadecimal strings easily readable for computers. This is why, GIS and spatial databases use WKB to transfer and store geometry objects. WKT, on the other hand, is a human-readable text markup description of simple features. Both formats are exchangeable, and if we present one, we will naturally choose the WKT representation. The basis for each geometry type is the point. A point is simply a coordinate in 2D, 3D or 4D space (see vignette(&quot;sf1&quot;) for more information) such as: POINT (5 2) A linestring is a sequence of points with a straight line connecting the points, for example: LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) A polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates. By definition, a polygon has one exterior boundary (outer ring) and can have zero or more interior boundaries (inner rings), also known as holes. Polygon without a hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) Polygon with one hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) So far we have created geometries with only one geometric entity per feature. However, sf also allows multiple geometries to exist within a single feature (hence the term ‘geometry collection’) using “multi” version of each geometry type: Multipoint - MULTIPOINT (5 2, 1 3, 3 4, 3 2) Multistring - MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) Multipolygon - MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2))) Finally, a geometry collection might contain any combination of geometry types: Geometry collection - GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))) 2.1.5.2 Simple feature geometry (sfg) objects In R, the sfg class represents the different simple feature geometry types: (multi-)point, (multi-)linestring, (multi-)polygon or geometry collection. Usually you are spared the tedious task of creating geometries on your own since you can simply import an already existing spatial file. However, there are a set of function to create simple feature geometry objects (sfg) from scratch if needed. The names of these functions are simple and consistent, as they all start with the st_ prefix and end with the name of the geometry type in lowercase letters: A point - st_point() A linestring - st_linestring() A polygon - st_polygon() A multipoint - st_multipoint() A multilinestring - st_multilinestring() A multipolygon - st_multipolygon() A geometry collection - st_geometrycollection() In R, you create sfg objects with the help of three native data types: A numeric vector - a single point A matrix - a set of points, where each row contains a point - a multipoint or linestring A list - any other set, e.g. a multilinestring or geometry collection To create point objects, we use the st_point() function in conjunction with a numeric vector: # note that we use a numeric vector for points st_point(c(5, 2)) # XY point #&gt; POINT (5 2) st_point(c(5, 2, 3)) # XYZ point #&gt; POINT Z (5 2 3) st_point(c(5, 2, 1), dim = &quot;XYM&quot;) # XYM point #&gt; POINT M (5 2 1) st_point(c(5, 2, 3, 1)) # XYZM point #&gt; POINT ZM (5 2 3 1) XY, XYZ and XYZM types of points are automatically created based on the length of a numeric vector. Only the XYM type needs to be specified using a dim argument. By contrast, use matrices in the case of multipoint (st_multipoint()) and linestring (st_linestring()) objects: # the rbind function simplifies the creation of matrices ## MULTIPOINT multipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2)) st_multipoint(multipoint_matrix) #&gt; MULTIPOINT (5 2, 1 3, 3 4, 3 2) ## LINESTRING linestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)) st_linestring(linestring_matrix) #&gt; LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) Finally, use lists for the creation of multilinestrings, (multi-)polygons and geometry collections: ## POLYGON polygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) st_polygon(polygon_list) #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) ## POLYGON with a hole polygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)) polygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4)) polygon_with_hole_list = list(polygon_border, polygon_hole) st_polygon(polygon_with_hole_list) #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) ## MULTILINESTRING multilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) st_multilinestring((multilinestring_list)) #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) ## MULTIPOLYGON multipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))), list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))) st_multipolygon(multipolygon_list) #&gt; MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2))) ## GEOMETRYCOLLECTION gemetrycollection_list = list(st_multipoint(multipoint_matrix), st_linestring(linestring_matrix)) st_geometrycollection(gemetrycollection_list) #&gt; GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)) 2.1.5.3 Simple feature collections One sfg object contains only a single simple feature geometry. A collection of simple feature geometries (sfc) is a list is a list of sfg objects and can additionally contain information about the coordinate reference system in use. For instance, to combine two simple features into one object with two features, we can use the st_sfc() function. This is important since this collection represents the geometry column in sf data frames: # sfc POINT point1 = st_point(c(5, 2)) point2 = st_point(c(1, 3)) st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; POINT (1 3) In most cases, an sfc object contains objects of the same geometry type. Therefore, when we convert sfg objects of type polygon into a simple feature collection, we would also end up with an sfc object of type polygon. Equally, a collection of multilinestrings would result in an sfc object of type multilinestring: # sfc POLYGON polygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) polygon1 = st_polygon(polygon_list) polygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))) polygon2 = st_polygon(polygon_list2) st_sfc(polygon1, polygon2) #&gt; Geometry set for 2 features #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) #&gt; POLYGON ((0 2, 1 2, 1 3, 0 3, 0 2)) # sfc MULTILINESTRING multilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) multilinestring1 = st_multilinestring((multilinestring_list1)) multilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), rbind(c(1, 7), c(3, 8))) multilinestring2 = st_multilinestring((multilinestring_list2)) st_sfc(multilinestring1, multilinestring2) #&gt; Geometry set for 2 features #&gt; geometry type: MULTILINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 7 ymax: 9 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... #&gt; MULTILINESTRING ((2 9, 7 9, 5 6, 4 7, 2 7), (1 ... It is also possible to create an sfc object from sfg objects with different geometry types: # sfc GEOMETRY st_sfc(point1, multilinestring1) #&gt; Geometry set for 2 features #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 5 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... As mentioned before, sfc objects can additionally store information on the coordinate reference systems (CRS). To specify a certain CRS, we can use the epsg (SRID) or proj4string attributes of an sfc object. The default value of epsg (SRID) and proj4string is NA (Not Available): st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; POINT (1 3) Of course, all geometries in an sfc object must have the same CRS. We can add coordinate reference system as a crs argument of st_sfc(). This argument accepts either an integer with the epsg code (e.g., 4326) or a proj4string character string (e.g., &quot;+proj=longlat +datum=WGS84 +no_defs&quot;) (see section 2.3). # EPSG definition st_sfc(point1, point2, crs = 4326) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) # PROJ4STRING definition st_sfc(point1, point2, crs = &quot;+proj=longlat +datum=WGS84 +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) For example, we can set the UTM Zone 11N projection with epsg code 2955: st_sfc(point1, point2, crs = 2955) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 2955 #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) As you can see above, the proj4string definition was automatically added. Now we can try to set the CRS using proj4string: st_sfc(point1, point2, crs = &quot;+proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) However, the epsg string of our result remained empty. This is because there is no general method to convert from proj4string to epsg. 2.1.5.4 Simple feature objects So far, we have only dealt with the pure geometries. Most of the time, however, these geometries come with a set of attributes describing them. These attributes could represent the name of the geometry, measured values, groups to which the geometry belongs, and many more. For example, we measured a temperature of 25°C on Trafalgar Square in London on June 21th 2017. Hence, we have a specific point in space (the coordinates), the name of the location (Trafalgar Square), a temperature value, the date of the measurement. Other attributes might include a urbanity category (city or village), or a remark if the measurement was made using an automatic station. The simple feature class, sf, is a combination of an attribute table (data.frame) and a simple feature geometry collection (sfc). Simple features are created using the st_sf() function: # sfg objects london_point = st_point(c(0.1, 51.5)) ruan_point = st_point(c(-9, 53)) # sfc object our_geometry = st_sfc(london_point, ruan_point, crs = 4326) # data.frame object our_attributes = data.frame(name = c(&quot;London&quot;, &quot;Ruan&quot;), temperature = c(25, 13), date = c(as.Date(&quot;2017-06-21&quot;), as.Date(&quot;2017-06-22&quot;)), category = c(&quot;city&quot;, &quot;village&quot;), automatic = c(FALSE, TRUE)) # sf object sf_points = st_sf(our_attributes, geometry = our_geometry) The above example illustrates the components of sf objects. Firstly, coordinates define the geometry of the simple feature geometry (sfg). Secondly, we can combine the geometries in a simple feature collection (sfc) which also stores the CRS. Subsequently, we store the attribute information on the geometries in a data.frame. Finally, the st_sf() function combines the attribute table and the sfc object in an sf object. sf_points #&gt; Simple feature collection with 2 features and 5 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: -9 ymin: 51.5 xmax: 0.1 ymax: 53 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name temperature date category automatic geometry #&gt; 1 London 25 2017-06-21 city FALSE POINT (0.1 51.5) #&gt; 2 Ruan 13 2017-06-22 village TRUE POINT (-9 53) class(sf_points) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; The result shows that sf objects actually have two classes, sf and data.frame. Simple features are simply data frames (square tables), but with spatial attributes (usually stored in a special geom list-column in the data frame). This duality is central to the concept of simple features: most of the time a sf can be treated as and behaves like a data.frame. Simple features are, in essence, data frames with a spatial extension. 2.2 Raster data The geographic raster data model consists of a raster header and a matrix (with rows and columns) representing equally spaced cells (often also called pixels; Figure 2.7:A). The raster header defines the coordinate reference system, the extent and the origin. The origin (or starting point) is frequently the coordinate of the lower-left corner of the matrix (the raster package, however, uses the upper left corner, by default (Figure 2.7:B)). The header defines the extent via the number of columns, the number of rows and the cell size resolution. Hence, starting from the origin, we can easily access and modify each single cell by either using the ID of a cell (Figure 2.7:B) or by explicitly specifying the rows and columns. This matrix representation avoids storing explicitly the coordinates for the four corner points (in fact it only stores one coordinate, namely the origin) of each cell corner as would be the case for rectangular vector polygons. This and map algebra makes raster processing much more efficient and faster than vector data processing. However, in contrast to vector data, a raster cell can only hold a single value. The value might be numeric or categorical (Figure 2.7:C). Figure 2.7: Raster data: A - a grid representation; B - numbers of the cells; C - values of the cells; D - a final raster map. Raster maps usually represent continuous phenomena such as elevation, temperature, population density or spectral data (Figure 2.8). Of course, we can represent discrete features such as soil or landcover classes also with the help of a raster data model (Figure 2.8). Consequently, the discrete borders of these features become blurred, and depending on the spatial task a vector representation might be more suitable. Figure 2.8: Examples of continuous (left) and categorical (right) raster. 2.2.1 An introduction to raster The raster package supports raster objects in R. It provides an extensive set of functions to create, read, export, manipulate and process raster datasets. Aside from general raster data manipulation, raster provides many low level functions that can form the basis to develop more advanced raster functionality. raster also lets you work on large raster datasets that are too large to fit into the main memory. In this case, raster provides the possibility to divide the raster into smaller chunks (rows or blocks), and processes these iteratively instead of loading the whole raster file into RAM (for more information, please refer to vignette(&quot;functions&quot;, package = &quot;raster&quot;). For the illustration of raster concepts, we will use datasets from the spDataLarge (note these packages were loaded at the beginning of the chapter). It consists of a few raster and one vector datasets covering an area of the Zion National Park (Utah, USA). For example, srtm.tif is a digital elevation model of this area (for more details - see its documentation ?srtm) First of all, we would like to create a RasterLayer object named new_raster: raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) new_raster = raster(raster_filepath) Typing the name of the raster into the console, will print out the raster header (extent, dimensions, resolution, CRS) and some additional information (class, data source name, summary of the raster values): new_raster #&gt; class : RasterLayer #&gt; dimensions : 463, 459, 212517 (nrow, ncol, ncell) #&gt; resolution : 73.7, 92.5 (x, y) #&gt; extent : 301929, 335757, 4111262, 4154089 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; data source : /home/travis/R/Library/spDataLarge/raster/srtm.tif #&gt; names : srtm #&gt; values : 1050, 2895 (min, max) To access individual header information, you can use following commands: # dimensions (number of rows, number of columns, number of cells) dim(new_raster) #&gt; [1] 463 459 1 # spatial resolution res(new_raster) #&gt; [1] 73.7 92.5 # spatial extent extent(new_raster) #&gt; class : Extent #&gt; xmin : 301929 #&gt; xmax : 335757 #&gt; ymin : 4111262 #&gt; ymax : 4154089 # coordinate reference system crs(new_raster) #&gt; CRS arguments: #&gt; +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m #&gt; +no_defs Note that in contrast to the sf package, raster only accepts the proj4string representation of the coordinate reference system. Sometimes it is important to know if all values of a raster are currently in memory or on disk. Find out with the inMemory() function: inMemory(new_raster) #&gt; [1] FALSE help(package = &quot;raster&quot;, topic = &quot;raster-package&quot;) returns a full list of all available raster functions. 2.2.2 Basic map making Similar to the sf package, raster also provides plot() methods for its own classes. plot(new_raster) Moreover, it is possible to plot a raster and overlay it with vector data. For this purpose, we need to read-in a vector dataset: vector_filepath = system.file(&quot;vector/zion.gpkg&quot;, package=&quot;spDataLarge&quot;) new_vector = st_read(vector_filepath) Our new object, new_vector, is a polygon representing the borders of Zion National Park (?zion). We can add the borders to the elevation map using the add argument of the plot(): plot(new_raster) plot(new_vector, add = TRUE) #&gt; Warning in plot.sf(new_vector, add = TRUE): ignoring all but the first #&gt; attribute There are several different approaches to plot raster data in R: You can use spplot() to visualize several (such as spatiotemporal) layers at once. You can also do so with the rasterVis package which provides more advanced methods for plotting raster objects. Packages such as tmap, mapview and leaflet facilitate especially interactive mapping of both raster and vector objects. 2.2.3 Raster classes The RasterLayer class represents the simplest form of a raster object, and consists of only one layer. The easiest way to create a raster object in R is to read-in a raster file from disk or from a server. raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) new_raster = raster(raster_filepath) The raster package support numerous drivers with the help of rgdal. To find out which drivers are available on your system, run raster::writeFormats() and rgdal::gdalDrivers(). Aside from reading in a raster, you can also create one from scratch. Running raster() creates an empty RasterLayer. Here, however, we will create manually a very simple raster. This should make it easy to understand how raster and related operations work. Our raster should consist of just three rows and columns centered around the null meridian and the equator (see xmn, xmx, ymn and ymx parameters). Additionally, we define a resolution of 0.5, which here corresponds to 0.5 degrees since the default proj4string of a raster object is WGS84. Finally, we set the values with the vals argument. Here, we just number the cells, that means we assign 1 to cell 1, 2 to cell 2, and finally 36 to cell 36. We know that there are 36 cells by multiplying six (rows) by six (columns). As we have seen above, setting raster values in R corresponds to a rowwise cell filling starting at the upper left corner. Consequently, the upper first row contains the values 1 to 6, the second row 7 to 12, and the last row 31 to 36. # creation of the RasterLayer object with a given number of columns and rows, and extent new_raster2 = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) For still further ways of creating a raster object have a look at the help file - ?raster. Aside from RasterLayer, there are two additional classes: RasterBrick and RasterStack. Both can handle multiple layers, but differ regarding the number of supported file formats, type of internal representation and processing speed. A RasterBrick consists of multiple layers, which typically correspond to a single multispectral satellite file or a single multilayer object in memory. The brick() function creates a RasterBrick object. Usually, you provide it with a filename to a multilayer raster file but might also use another raster object and other spatial objects (see its help page for all supported formats). multilayer_raster_filepath = system.file(&quot;raster/landsat.tif&quot;, package=&quot;spDataLarge&quot;) r_brick = brick(multilayer_raster_filepath) r_brick #&gt; class : RasterBrick #&gt; dimensions : 1428, 1128, 1610784, 4 (nrow, ncol, ncell, nlayers) #&gt; resolution : 30, 30 (x, y) #&gt; extent : 301905, 335745, 4111245, 4154085 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : /home/travis/R/Library/spDataLarge/raster/landsat.tif #&gt; names : landsat.1, landsat.2, landsat.3, landsat.4 #&gt; min values : 7550, 6404, 5678, 5252 #&gt; max values : 19071, 22051, 25780, 31961 The nlayers function retrieves the number of layers stored in a Raster* object: nlayers(r_brick) #&gt; [1] 4 A RasterStack is similar to a RasterBrick in the sense that it consists also of multiple layers. However, in contrast to RasterBrick, RasterStack allows to connect several raster objects stored in different files or multiply objects in memory. More specifically, a RasterStack is a list of RasterLayer objects with the same extent and resolution. Hence, one way to create it is with the help of spatial objects already existing in R’s global environment. And again, one can simply specify a path to a file stored on disk. raster_on_disk = raster(r_brick, layer = 1) raster_in_memory = raster(xmn = 301905, xmx = 335745, ymn = 4111245, ymx = 4154085, res = 30) values(raster_in_memory) = sample(1:ncell(raster_in_memory)) crs(raster_in_memory) = crs(raster_on_disk) r_stack &lt;- stack(raster_in_memory, raster_on_disk) r_stack #&gt; class : RasterStack #&gt; dimensions : 1428, 1128, 1610784, 2 (nrow, ncol, ncell, nlayers) #&gt; resolution : 30, 30 (x, y) #&gt; extent : 301905, 335745, 4111245, 4154085 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 #&gt; names : layer, landsat.1 #&gt; min values : 1, 7550 #&gt; max values : 1610784, 19071 Another difference is that the processing time for RasterBrick objects is usually shorter than for RasterStack objects. Decision on which Raster* class should be used depends mostly on a character of input data. Processing of a single mulitilayer file or object is the most effective with RasterBrick, while RasterStack allows calculations based on many files, many Raster* objects, or both. Operations on RasterBrick and RasterStack objects will typically return a RasterBrick. 2.3 Coordinate Reference Systems Vector and raster spatial data types share concepts intrinsic to spatial data. Perhaps the most fundamental of these is the Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other bodies). Coordinate system could be either geographic or projected (Figures 2.9 and 2.10). 2.3.1 Geographic coordinate systems Geographic coordinate systems identify any location on the Earth’s surface using two values - longitude and latitude. The first one is an angle from the prime meridian plan and the second one is an angle from the equatorial plane to this location. Therefore, units of geographic coordinate systems are degrees. The surface of the Earth in geographic coordinate systems is represented by a spherical or ellipsoidal surface. Spherical models represent the shape of the Earth of a specific radius, assuming that the Earth is a perfect sphere. While it simplifies calculations and works well for small scale maps, it could not be sufficient for the work at a larger scale. More accurate measurements can be done with an ellipsoidal model. The shape of an ellipse is defined by either the equatorial radius (a) and the polar radius (b), or by a and the inverse flattening (rf), where: \\[rf = 1/((a-b)/a)\\] Since the Earth is flattened at the poles, an equatorial radius is slightly longer than a polar axis. For example, the difference of the equatorial radius and polar radius in the WGS 84 ellipsoid is about 21.385 km. You can access a list of available ellipses and theirs properties using the st_proj_info(type = &quot;ellps&quot;) function. Additionally, a position and orientation of the spheroid relative to the center of the Earth needs to be defined using a datum. The Earth’s surface is irregular due to gravitational and surface feature variations. Therefore, datums were created to account for the local variations in establishing a coordinate system. There are two types of datums - local and geocentric. In local datums, the ellipsoid surface aligns closely to the Earth surface at a particular location. For example, NAD27 (North American Datum of 1927) is a local datum created for the United States area. Geocentric datums are aligned to the center of the Earth It includes WGS84 (World Geodetic System 1984) Datum. A list of datums supported in R could be obtain with st_proj_info(type = &quot;datum&quot;). 2.3.2 Projected coordinate systems Projected coordinate systems are based on Cartesian coordinates (X, Y) and represent any area on a flat surface. A projected coordinate system has an origin, x and y axes, and a linear unit of measure. All projected coordinate systems are based on geographic coordinate systems. Maps projections are mathematical models for conversion of three-dimensional surface into a two-dimensional representation on a map. This transition cannot be done without adding some distortion. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve: equal-area preserves area, azimuthal preserve direction, equidistant preserve distance, and conformal preserve local shape. There are three main groups of projection types - conic, cylindrical, and planar. In a conic projection the Earth surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines in this projection. Therefore, it is the best suited for maps of mid-latitude areas. A cylindrical projection maps the surface onto a cylinder. This projection also could be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are the most often used in mapping of the entire world. A planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions. st_proj_info(type = &quot;proj&quot;) gives a list of the available projections supported by the PROJ.4 library. 2.3.3 CRS in R Two main ways to describe CRS in R are an epsg code or a proj4string definition. Both of these approaches have advantages and disadvantages. An epsg code is usually shorter, and therefore easier to remember. The code also refers to only one, well-defined coordinate reference system. On the other hand, a proj4string definition allows you more flexibility when it comes to specifying different parameters such as the projection type, the datum and the ellipsoid.17 This way you can specify many different projections, and modify existing ones. This makes the proj4string approach also more complicated. epsg points to exactly one particular CRS. Spatial R packages support a wide range of CRSs they use long-establish proj4 library. Other than searching for EPSG codes on-line, another quick way to find-out about available CRSs is via the rgdal::make_EPSG() function, which outputs a data frame of available projections. Before going into more detail into these, it’s worth learning how to view and filter them inside R, as this could save time trawling the internet. The following code will show available CRSs interactively, allowing filtering of ones of interest (try filtering for the OSGB CRSs for example): crs_data = rgdal::make_EPSG() View(crs_data) In sf the CRS of an object can be retrieved using st_crs(): st_crs(new_vector) # get CRS #&gt; Coordinate Reference System: #&gt; No EPSG code #&gt; proj4string: &quot;+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; In cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the st_set_crs() function can be used: new_vector = st_set_crs(new_vector, 26912) # set CRS The warning message informs that the st_set_crs() function do not transform data from one CRS to another. Figure 2.9: Examples of geographic (WGS 84; left) and projected (NAD83 / UTM zone 12N; right) and coordinate systems for a vector data type. The projection() function can be use to access a CRS information from the Raster* object: projection(new_raster) # get CRS #&gt; [1] &quot;+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; The same function, projection(), is used to set a CRS for raster objects. The main difference, comparing to vector data, is that raster objects accepts only proj4 definitions: projection(new_raster) = &quot;+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; # set CRS Figure 2.10: Examples of geographic (WGS 84; left) and projected (NAD83 / UTM zone 12N; right) and coordinate systems for a raster data type More information on CRS and spatial tranformation is in chapter 6. 2.4 Units An important feature of CRSs is that they contain information about spatial units. Clearly it is vital to know whether a house’s measurements are in feet or meters, and the same applies to maps. It is good cartographic practice to add a scale bar onto maps to demonstrate the relationship between distances on the page or screen and distances on the ground. Likewise, it is important to formally specify the units in which the geometry data or pixels are measured to provide context, and ensure that subsequent calculations are done in context. A novel feature of geometry data in sf objects is that they have native support for units. This means that distance, area and other geometric calculations in sf return values that come with a units attribute, defined by the units package (Pebesma, Mailund, and Hiebert 2016). This is advantageous because it prevents confusion caused by the fact that different CRSs use different units (most use meters, some use feet). Furthermore, it also provides information on dimensionality, as illustrated by the following calculation which reports the area of Nigeria: nigeria = world[world$name_long == &quot;Nigeria&quot;, ] st_area(nigeria) #&gt; 9.05e+11 m^2 The result is in units of square meters (m2), showing a) that the result represents two-dimensional space and b) and that Nigeria is a large country! This information, stored as an attribute (which interested readers can discover with attributes(st_area(nigeria))) is advantageous for many reasons, for example it could feed into subsequent calculations such as population density. Reporting units prevents confusion. To take the Nigeria example, if the units remained unspecified, one could incorrectly assume that the units were in km2. To translate the huge number into a more digestible size, it is tempting to divide the results by a million (the number of square meters in a square kilometer): st_area(nigeria) / 1e6 #&gt; 905072 m^2 However, the result is incorrectly given again as square meters. The solution is to set the correct units with the units package: units::set_units(st_area(nigeria), km^2) #&gt; 905072 km^2 Units are of equal importance in the case of raster data. However, so far sf is the only spatial package that supports units, meaning that people working on raster data should approach changes in the units of analysis (e.g., converting pixel widths from imperial to decimal units) with care. The new_raster object (see above) uses a UTM projection with meters as units. Consequently, its resolution is also given in meters but you have to know it, since the res() function simply returns a numeric vector. res(new_raster) #&gt; [1] 73.7 92.5 If we used the WGS84 projection, the units would change. repr = projectRaster(new_raster, crs = &quot;+init=epsg:4326&quot;) res(repr) #&gt; [1] 0.000831 0.000833 Again, the res() command gives back a numeric vector without any unit, forcing us to know that the unit of the WGS84 projection is decimal degrees. 2.5 Exercises What does the summary of the geometry column tell us about the world dataset, in terms of: The geometry type? How many countries there are? The coordinate reference system (CRS)? Using sf’s plot() command, create a map of Nigeria in context, building on the code that creates and plots Asia above (see Figure 2.5 for an example of what this could look like). Hint: this used the lwd, main and col arguments of plot(). Bonus: make the country boundaries a dotted grey line. Hint: border is an additional argument of plot() for sf objects. What does the cex argument do in the plot() function that generates Figure 2.6? Why was cex passed the sqrt(world$pop) / 10000 instead of just the population directly? Bonus: what equivalent arguments to cex exist in the dedicated vizualisation package tmap? Re-run the code that ‘generated’ Figure 2.6 at the end of 2.1.4 and find 3 similarities and 3 differences between the plot produced on your computer and that in the book. What is similar? What has changed? Bonus: play around with and research base plotting arguments to make your version of Figure 2.6 more attractive. Which arguments were most useful. Advanced: try to reproduce the map presented in Figure 2.1.4. Copy-and-pasting is prohibited! Read the raster/nlcd2011.tif file from the spDataLarge package. What kind of information can you get about the properties of this file? Create an empty RasterLayer object called my_raster with 10 columns and 10 rows, and resolution of 10 units. Assign random values between 0 and 10 to the new raster and plot it. References "],
["attr.html", "3 Attribute data operations Prerequisites 3.1 Introduction 3.2 Vector attribute manipulation 3.3 Manipulating raster objects 3.4 Exercises", " 3 Attribute data operations Prerequisites This chapter requires the packages tidyverse, sf and raster: library(sf) library(raster) library(tidyverse) It also relies on spData, which loads world, worldbank_df and us_states datasets: library(spData) 3.1 Introduction Attribute data is non-spatial information associated with geographic (geometry) data. A bus stop provides a simple example. In a spatial vector object its position would typically be represented by latitude and longitude coordinates (geometry data), in addition to its name. The name is an attribute of the feature (to use Simple Features terminology) that bears no relation to its geometry. Another example is the elevation value (attribute) for a specific grid cell in raster data. Unlike vector data, the raster data model stores the coordinate of the grid cell only indirectly: There is a less clear distinction between attribute and spatial information in raster data. Say, we are in the 3rd row and the 4th column of a raster matrix. To derive the corresponding coordinate, we have to move from the origin three cells in x-direction and four cells in y-direction with the cell resolution defining the distance for each x- and y-step. The raster header gives the matrix a spatial dimension which we need when plotting the raster or when we want to combine two rasters, think, for instance, of adding the values of one raster to another (see also next Chapter). This chapter focuses on non-geographic operations on vector and raster data. For vector data, we will introduce subsetting, aggregating and joining attribute data in the next section. Note that the corresponding functions also have a geographic equivalent. Sometimes you can even use the same functions for attribute and spatial operations. This is the case for subsetting as base R’s [ and tidyverse’s filter() let you also subset spatial data based on the spatial extent of another spatial object (see Chapter 4). Therefore the skills you learn here are cross-transferable which is also why this chapter lays the foundation for the next chapter (Chapter 4) which extends the here presented methods to the spatial world. Raster attribute data operations are covered in Section 3.3, which covers the creating continuous and categorical raster layers and extracting cell values from one layer and multiple layers (raster subsetting). Section 3.3.2 provides an overview of ‘global’ raster operations which can be used to characterize entire raster datasets. 3.2 Vector attribute manipulation Geographic vector data in R are well-support by sf, a class which extends the data.frame. Thus sf objects have one column per attribute variable (such as ‘name’) and one row per observation, or feature (e.g. per bus station). sf objects also have a special column to contain geometry data, usually named geometry. The geometry column is special because it is a list-colum, which can contain multiple geographic entities (points, lines, polygons) per row. In Chapter 2 we saw how to perform generic methods such as plot() and summary() on sf objects. sf also provides methods that allow sf objects to behave like regular data frames: methods(class = &quot;sf&quot;) # methods for sf objects, first 12 shown #&gt; [1] aggregate cbind coerce #&gt; [4] initialize merge plot #&gt; [7] print rbind [ #&gt; [10] [[&lt;- $&lt;- show Many of these functions, including rbind() (for binding rows of data together) and $&lt;- (for creating new columns) were developed for data frames. A key feature of sf objects is that they store spatial and non-spatial data in the same way, as columns in a data.frame (the geometry column is typically called geometry). The geometry column of sf objects is typically called geometry but any name can be used. The following command, for example, creates a geometry column named g: st_sf(data.frame(n = world$name_long), g = world$geom) This enables geometries imported from spatial databases to have a variety of names such as wkb_geometry and the_geom. sf objects also support tibble and tbl classes used in the tidyverse, allowing ‘tidy’ data analysis workflows for spatial data. Thus sf enables the full power of R’s data analysis capabilities to be unleashed on geographic data. Before using these capabilities it’s worth re-capping how to discover the basic properties of vector data objects. Let’s start by using base R functions for to get a measure of the world dataset: dim(world) # it is a 2 dimensional object, with rows and columns #&gt; [1] 177 11 nrow(world) # how many rows? #&gt; [1] 177 ncol(world) # how many columns? #&gt; [1] 11 Our dataset contains ten non-geographic columns (and one geometry list-column) with almost 200 rows representing the world’s countries. Extracting the attribute data of an sf object is the same as removing its geometry: world_df = st_set_geometry(world, NULL) class(world_df) #&gt; [1] &quot;data.frame&quot; This can be useful if the geometry column causes problems, e.g., by occupying large amounts of RAM, or to focus the attention on the attribute data. For most cases, however, there is no harm in keeping the geometry column because non-spatial data operations on sf objects only change an object’s geometry when appropriate (e.g. by dissolving borders between adjacent polygons following aggregation). This means that proficiency with attribute data in sf objects equates to proficiency with data frames in R. For many applications, the tidyverse package dplyr offers the most effective and intuitive approach of working with data frames, hence the focus on this approach in this section.18 3.2.1 Vector attribute subsetting Base R subsetting functions include [, subset() and $. dplyr subsetting functions include select(), filter(), and pull(). Both sets of functions preserve the spatial components of attribute data in sf objects. The [ operator can subset both rows and columns. You use indices to specify the elements you wish to extract from an object, e.g., object[i, j], with i and j typically being numbers or logical vectors — TRUEs and FALSEs — representing rows and columns (they can also be character strings, indicating row or column names). Leaving i or j empty returns all rows or columns, so world[1:5, ] returns the first five rows and all columns. The examples below demonstrate subsetting with base R. The results are not shown; check the results on your own computer: world[1:6, ] # subset rows by position world[, 1:3] # subset columns by position world[, c(&quot;name_long&quot;, &quot;lifeExp&quot;)] # subset columns by name A demonstration of the utility of using logical vectors for subsetting is shown in the code chunk below. This creates a new object, small_countries, containing nations whose surface area is smaller than 10,000 km2: sel_area = world$area_km2 &lt; 10000 summary(sel_area) # a logical vector #&gt; Mode FALSE TRUE #&gt; logical 170 7 small_countries = world[sel_area, ] The intermediary sel_object is a logical vector that shows that only seven countries match the query. A more concise command, that omits the intermediary object, generates the same result: small_countries = world[world$area_km2 &lt; 10000, ] Another the base R function subset() provides yet another way to achieve the same result: small_countries = subset(world, area_km2 &lt; 10000) You can use the $ operator to select a specific variable by its name. The result is a vector: world$name_long Base R functions are mature and widely used. However, the more recent dplyr approach has several advantages. It enables intuitive workflows. It’s fast, due to its C++ backend and database integration, important when working with big data. The main dplyr subsetting functions are select(), slice(), filter() and pull(). raster and dplyr packages have a function called select(). If both packages are loaded, this can generate error messages containing the text: unable to find an inherited method for function ‘select’ for signature ‘“sf”’. To avoid this error message, and prevent ambiguity, we use the long-form function name, prefixed by the package name and two colons (usually omitted from R scripts for concise code): dplyr::select(). select() selects columns by name or position. For example, you could select only two columns, name_long and pop, with the following command (note the geom column remains): world1 = dplyr::select(world, name_long, pop) names(world1) #&gt; [1] &quot;name_long&quot; &quot;pop&quot; &quot;geom&quot; select() also allows subsetting of a range of columns with the help of the : operator: # all columns between name_long and pop (inclusive) world2 = dplyr::select(world, name_long:pop) Omit specific columns with the - operator: # all columns except subregion and area_km2 (inclusive) world3 = dplyr::select(world, -subregion, -area_km2) Conveniently, select() lets you subset and rename columns at the same time, for example: world4 = dplyr::select(world, name_long, population = pop) names(world4) #&gt; [1] &quot;name_long&quot; &quot;population&quot; &quot;geom&quot; This is more concise than the base R equivalent: world5 = world[, c(&quot;name_long&quot;, &quot;pop&quot;)] # subset columns by name names(world5)[2] = &quot;population&quot; # rename column manually select() also works with ‘helper functions’ for advanced subsetting operations, including contains(), starts_with() and num_range() (see the help page with ?select for details). slice() is the row-equivalent of select(). The following code chunk, for example, selects the 3rd to 5th rows: slice(world, 3:5) filter() is dplyr’s equivalent of base R’s subset() function. It keeps only rows matching given criteria, e.g., only countries with a very high average of life expectancy: # Countries with a life expectancy longer than 82 years world6 = filter(world, lifeExp &gt; 82) The standard set of comparison operators can be used in the filter() function, as illustrated in Table 3.1: Table 3.1: Table of comparison operators that result in boolean (TRUE/FALSE) outputs. Symbol Name == Equal to != Not equal to &gt;, &lt; Greater/Less than &gt;=, &lt;= Greater/Less than or equal &amp;, |, ! Logical operators: And, Or, Not A benefit of dplyr is its compatibility with the pipe operator %&gt;%. This ‘R pipe’, which takes its name from the Unix pipe | and is part of the magrittr package, enables expressive code by ‘piping’ the output of a previous command into the first argument of the next function. This allows chaining data analysis commands, with the data frame being passed from one function to the next. The dplyr function pull() extracts a single variable as a vector. This is illustrate below, in which the world dataset is subset by columns (name_long and continent) and the first five rows (result not shown). world7 = world %&gt;% dplyr::select(name_long, continent) %&gt;% slice(1:5) The above chunk shows how the pipe operator allows commands to be written in a clear order: the above run from top to bottom (line-by-line) and left to right. Without %&gt;% one would be forced to create intermediary objects or use nested function calls, e.g.: world8 = dplyr::select( slice(world, 1:5), name_long, continent) This generates the same result — verify this with identical(world7, world8) — in the same number of lines of code, but in a much more confusing way, starting with the function that is called last! There are additional advantages of pipes from a communication perspective: they encourage adding comments to self-contained functions and allow single lines commented-out without breaking the code. 3.2.2 Vector attribute aggregation Aggregation operations summarize datasets by a grouping variable, which can be either another attribute column or a spatial object. Imagine we would like to calculate the number of people per continent. Fortunately, our world dataset has the necessary ingredients, with the pop column containing the population per country and the grouping variable continent. In base R this can be done with aggregate() as follows: aggregate(pop ~ continent, FUN = sum, data = world, na.rm = TRUE) The result is a non-spatial data frame with six rows, one per continent, and two columns (see Table 3.2 with results for the top 3 most populous continents). summarize() is the dplyr equivalent of aggregate(), which uses the function group_by() to create the grouping variable. The tidy equivalent of the aggregate() method is as follows: group_by(world, continent) %&gt;% summarize(pop = sum(pop, na.rm = TRUE)) This approach is flexible, allowing the resulting columns to be named. Further, omitting the grouping variable puts everything on in one group. This means summarize() can be used to calculate Earth’s total population (~7 billion) and number of countries: world %&gt;% summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) #&gt; pop n_countries #&gt; 1 7.21e+09 177 The result is a spatial data frame of class sf (only the non-spatial results are shown): the aggregation procedure dissolves boundaries within continental land masses. In the previous code chunk pop and n_countries are column names in the result. sum() and n() were the aggregating functions. Let’s combine what we’ve learned so far about dplyr by chaining together functions to find the world’s 3 most populous continents (with dplyr::n() ) and the number of countries they contain. The output of the following code is presented in Table 3.2): world %&gt;% dplyr::select(pop, continent) %&gt;% group_by(continent) %&gt;% summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) %&gt;% top_n(n = 3, wt = pop) %&gt;% st_set_geometry(value = NULL) Table 3.2: The top 3 most populous continents, and the number of countries in each. continent pop n_countries Africa 1.15e+09 51 Asia 4.31e+09 47 Europe 7.39e+08 39 3.2.3 Vector attribute joining Combining data from different sources is a common task in data preparation. Joins do this by combining tables based on a shared ‘key’ variable. dplyr has powerful functions for joining: left_join(), right_join(), inner_join(), full_join, semi_join() and anti_join(). These function names follow conventions used in the database language SQL (Grolemund and Wickham 2016, Chapter 13). Using them with sf objects is the focus of this section. dplyr join functions work the same on data frames and sf objects, the only important difference being the geometry list column. The result of data joins can be either an sf or data.frame object. Most joins involving spatial data will have an sf object as the first argument and a data.frame object as the second argument, resulting in a new sf object (the reverse order is also possible and will return a data.frame). We will focus on the commonly used left and inner joins, which use the same syntax as the other join types (see Grolemund and Wickham 2016 for more join types). The easiest way to understand the concept of joins is to show how they work with a smaller dataset. We will use an sf object north_america with country codes (iso_a2), names and geometries, as well as a data.frame object wb_north_america containing information about urban population and unemployment for three countries. Note that north_america contains data about Canada, Greenland and the United States but the World Bank dataset (wb_north_america) contains information about Canada, Mexico and the United States: north_america = world %&gt;% filter(subregion == &quot;Northern America&quot;) %&gt;% dplyr::select(iso_a2, name_long) north_america$name_long #&gt; [1] &quot;Canada&quot; &quot;Greenland&quot; &quot;United States&quot; wb_north_america = worldbank_df %&gt;% filter(name %in% c(&quot;Canada&quot;, &quot;Mexico&quot;, &quot;United States&quot;)) %&gt;% dplyr::select(name, iso_a2, urban_pop, unemploy = unemployment) We will use a left join to combine the two datasets. Left joins are the most commonly used operation for adding attributes to spatial data, as they return all observations from the left object (north_america) and the matched observations from the right object (wb_north_america) in new columns. Rows in the left object without matches in the right (Greenland in this case) result in NA values. To join two objects we need to specify a key. This is a variable (or a set of variables) that uniquely identifies each observation (row). The by argument of dplyr’s join functions lets you identify the key variable. In simple cases, a single, unique variable exist in both objects like the iso_a2 column in our example (you may need to rename columns with identifying information for this to work): left_join1 = north_america %&gt;% left_join(wb_north_america, by = &quot;iso_a2&quot;) This has created a spatial dataset with the new variables added. The utility of this is shown in Figure 3.1, which shows the unemployment rate (a World Bank variable) across the countries of North America. Figure 3.1: The unemployment rate (taken from World Bank statistics) in Canada and the United States to illustrate the utility of joining attribute data on to spatial datasets. It is also possible to join objects by different variables. Both of the datasets have variables with names of countries, but they are named differently. The north_america has a name_long column and the wb_north_america has a name column. In these cases a named vector, such as c(&quot;name_long&quot; = &quot;name&quot;), can specify the connection: left_join2 = north_america %&gt;% left_join(wb_north_america, by = c(&quot;name_long&quot; = &quot;name&quot;)) names(left_join2) #&gt; [1] &quot;iso_a2.x&quot; &quot;name_long&quot; &quot;iso_a2.y&quot; &quot;urban_pop&quot; &quot;unemploy&quot; &quot;geom&quot; Note that the result contains two duplicated variables - iso_a2.x and iso_a2.y because both x and y objects have the column iso_a2. This can be solved by specifying all the keys: left_join3 = north_america %&gt;% left_join(wb_north_america, by = c(&quot;iso_a2&quot;, &quot;name_long&quot; = &quot;name&quot;)) Joins also work when a data frame is the first argument. This keeps the geometry column but drops the sf class, returning a data.frame object. # keeps the geom column, but drops the sf class left_join4 = wb_north_america %&gt;% left_join(north_america, by = c(&quot;iso_a2&quot;)) class(left_join4) #&gt; [1] &quot;data.frame&quot; In most cases the geometry column is only useful in an sf object. The geometry column can only be used for creating maps and spatial operations if R ‘knows’ it is a spatial object, defined by a spatial package such as sf. Fortunately non-spatial data frames with a geometry list column (like left_join4) can be coerced into an sf object as follows: st_as_sf(left_join4). In contrast to left_join(), inner_join() keeps only observations from the left object (north_america) where there are matching observations in the right object (wb_north_america). All columns from the left and right object are still kept: inner_join1 = north_america %&gt;% inner_join(wb_north_america, by = c(&quot;iso_a2&quot;, &quot;name_long&quot; = &quot;name&quot;)) inner_join1$name_long #&gt; [1] &quot;Canada&quot; &quot;United States&quot; 3.2.4 Creating attributes and removing spatial information Often, we would like to create a new column based on already existing columns. For example, we want to calculate population density for each country. For this we need to divide a population column, here pop, by an area column , here area_km2 with unit area in square km. Using base R, we can type: world_new = world # do not overwrite our original data world_new$pop_dens = world_new$pop / world_new$area_km2 Alternatively, we can use one of dplyr functions - mutate() or transmute(). mutate() adds new columns at the penultimate position in the sf object (the last one is reserved for the geometry): world %&gt;% mutate(pop_dens = pop / area_km2) The difference between mutate() and transmute() is that the latter skips all other existing columns (except for the sticky geometry column): world %&gt;% transmute(pop_dens = pop / area_km2) Existing columns could be also paste together using unite(). For example, we want to stick together continent and region_un columns into a new con_reg column. We could specify a separator to use between values and if input columns should be removed: world_unite = world %&gt;% unite(&quot;con_reg&quot;, continent:region_un, sep = &quot;:&quot;, remove = TRUE) The separate() function is the complement of the unite() function. Its role is to split one column into multiple columns using either a regular expression or character position. world_separate = world_unite %&gt;% separate(con_reg, c(&quot;continent&quot;, &quot;region_un&quot;), sep = &quot;:&quot;) Two helper functions, rename() and set_names can be used to change columns names. The first one, rename() replace an old name with a new one. For example, we want to change a name of column from name_long to name: world %&gt;% rename(name = name_long) set_names can be used to change names of many columns. In this function, we do not need to provide old names: new_names = c(&quot;ISO_A2&quot;, &quot;Name&quot;, &quot;Continent&quot;, &quot;Region&quot;, &quot;Subregion&quot;, &quot;Country_type&quot;, &quot;Area_in_km2&quot;, &quot;Population&quot;, &quot;Life_Expectancy&quot;, &quot;GDP_per_capita&quot;, &quot;geom&quot;) world %&gt;% set_names(new_names) It is important to note that the attribute data operations preserve the geometry of the simple features. As mentioned at the outset of the chapter, however, it can be useful to remove the geometry. Do do this, you have to explicitly remove it because sf explicitly makes the geometry column sticky. This behavior ensures that data frame operations do not accidentally remove the geometry column. Hence, an approach such as select(world, -geom) will be unsuccessful instead use st_set_geometry()19. world_data = world %&gt;% st_set_geometry(NULL) class(world_data) #&gt; [1] &quot;data.frame&quot; More details are provided in the help pages (which can be accessed via ?summarize and vignette(package = &quot;dplyr&quot;) and Chapter 5 of R for Data Science. 3.3 Manipulating raster objects In contrast to the vector data model underlying simple features (which represents points, lines and polygons as discrete entities in space), raster data represent continuous surfaces. This section shows how raster objects work, by creating them from scratch, building on section 2.2.1. Because of their unique structure, subsetting and other operations on raster datasets work in a different way, as demonstrated in section 3.3.1. The following code recreates the raster dataset used in section 2.2.3, the result of which is illustrated in Figure 3.2. This demonstrates how the raster() function works to create an example raster named elev (representing elevations). elev = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) The result is a raster object with 6 rows and 6 columns (specified by the nrow and ncol arguments), and minimum and a minimum and maximum spatial extent (specified by xmn, xmx and equivalent arguments for the y axis). The vals argument sets the values that each cell contains: numeric data ranging from 1 to 36 in this case. Raster objects can also contain categorical values of class logical or factor variables in R. The following code creates a raster representing grain sizes (Figure 3.2): grain_order = c(&quot;clay&quot;, &quot;silt&quot;, &quot;sand&quot;) grain_char = sample(grain_order, 36, replace = TRUE) grain_fact = factor(grain_char, levels = grain_order) grain = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = grain_fact) raster objects can contain values of class numeric, integer, logical or factor, but not character. To use character values they must first be converted into an appropriate class, for example using the function factor(). The levels argument was used in the preceding code chunk to create an ordered factor: clay &lt; silt &lt; sand in terms of grain size. See the Data structures chapter of (Wickham 2014) for further details on classes. raster objects represent categorical variables as integers, so grain[1, 1] returns a number that represents a unique identifiers, rather than “clay”, “silt” or “sand”. The raster object stores the corresponding look-up table or “Raster Attribute Table” (RAT) as a data frame in a new slot named attributes, which can be viewed with ratify(grain) (see ?ratify() for more information). Use the function levels() to retrieve the attribute table and add additional factor values: levels(grain)[[1]] = cbind(levels(grain)[[1]], wetness = c(&quot;wet&quot;, &quot;moist&quot;, &quot;dry&quot;)) levels(grain) #&gt; [[1]] #&gt; ID VALUE wetness #&gt; 1 1 clay wet #&gt; 2 2 silt moist #&gt; 3 3 sand dry This behavior demonstrates that raster cells can only possess one value, an identifier which can be used to look up the attributes in the corresponding attribute table (stored in a slot named attributes). This is illustrated in command below, which returns the grain size and wetness of cell IDs 1, 12 and 36, we can run: factorValues(grain, grain[c(1, 12, 36)]) #&gt; VALUE wetness #&gt; 1 sand dry #&gt; 2 clay wet #&gt; 3 silt moist Figure 3.2: Raster with numberic values (left) and a raster with categorical values (right). 3.3.1 Raster subsetting Raster subsetting is done with the base R operator [, which accepts a variety of inputs: row-column indexing cell IDs coordinates another raster object The latter two represent spatial subsetting (see the next chapter). The first two subsetting options are demonstrated in the commands below — both return the value of the top left pixel in the raster object elev (results not shown): # row 1, column 1 elev[1, 1] # cell ID 1 elev[1] To extract all values or complete rows, you can use values() and getValues(). For multi-layered raster objects stack or brick, this will return the cell value(s) for each layer. For example, stack(elev, grain)[1] returns a matrix with one row and two columns — one for each layer. For multi-layer raster objects another way to subset is with raster::subset(), which extracts layers from a raster stack or brick. The [[ and $ operators can also be used: r_stack = stack(elev, grain) names(r_stack) = c(&quot;elev&quot;, &quot;grain&quot;) # three ways to extract a layer of a stack raster::subset(r_stack, &quot;elev&quot;) r_stack[[&quot;elev&quot;]] r_stack$elev Cell values can be modified by overwriting existing values in conjunction with a subsetting operation. The following code chunk, for example, sets the upper left cell of elev to 0: elev[1, 1] = 0 elev[] #&gt; [1] 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #&gt; [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 Leaving the square brackets empty is a shortcut version of values() for retrieving all values of a raster. Multiple cells can also be modified in this way: elev[1, 1:2] = 0 3.3.2 Summarizing raster objects raster contains functions for extracting descriptive statistics for entire rasters. Printing a raster object to the console by default, by typing its name, returns minimum and maximum values of a raster. summary() provides common descriptive statistics (minimum, maximum, interquartile range and number of NAs). Further summary operations such as the standard deviation (see below) or custom summary statistics can be calculated with cellStats(). cellStats(elev, sd) If you provide the summary() and cellStats() functions with a raster stack or brick object, they will summarize each layer separately, as can be illustrated by running summary(brick(elev, grain)). Raster value statistics can be visualized in a variety of ways. Raster values, e.g. returned by values() or getValues(), can be plotted in a variety of ways, including with boxplot(), density(), hist() and pairs(), which work for raster objects, as demonstrated in the histogram created with the command below (not shown): hist(elev) Descriptive raster statistics belong to the so-called global raster operations. These and other typical raster processing operations are part of the map algebra scheme which are covered in the next chapter. Some function names clash between packages (e.g., select, as discussed in a previous note). In addition to not loading packages by referring to functions verbosely (e.g., dplyr::select()) another way to prevent function names clashes is by unloading the offending package with detach(). The following command, for example, unloads the raster package (this can also be done in the package tab in the right-bottom pane in RStudio): detach(“package:raster”, unload = TRUE, force = TRUE). The force argument makes sure that the package will be detached even if other packages depend on it. This, however, may lead to a restricted usability of packages depending on the detached package, and is therefore not recommended. 3.4 Exercises For these exercises we will use the us_states and us_states_df datasets from the spData package: library(spData) data(us_states) data(us_states_df) us_states is a spatial object (of class sf), containing geometry and a few attributes (including name, region, area, and population) of states within the contiguous United States. us_states_df is a data frame (of class data.frame) containing the name and additional variables (including median income and poverty level, for years 2010 and 2015) of US states, including Alaska, Hawaii and Puerto Rico. The data comes from the US Census Bureau, and is documented in ?us_states and ?us_states_df. Create a new object called us_states_name that contains only the NAME column from the us_states object. What is the class of the new object? Select columns from the us_states object which contain population data. Obtain the same result using a different command (bonus: try to find three ways of obtaining the same result). Hint: try to use helper functions, such as contains or starts_with from dplyr (see ?contains). Find all states with the following characteristics (bonus find and plot them): belong to the Midwest region. belong to the West region, have an area below 250,000 km2 and in 2015 a population greater than 5,000,000 residents (hint: you may need to use the function units::set_units() or as.numeric()). belong to the South region, had an area larger than 150,000 km2 or a total population in 2015 larger than 7,000,000 residents. What was the total population in 2015 in the us_states dataset? What was the minimum and maximum total population in 2015? How many states are there in each region? What was the minimum and maximum total population in 2015 in each region? What was the total population in 2015 in each region? Add variables from us_states_df to us_states, and create a new object called us_states_stats. What function did you use and why? Which variable is the key in both datasets? What is the class of the new object? us_states_df has two more variables than us_states. How you can find them? (hint: try to use the dplyr::anti_join function) What was the population density in 2015 in each state? What was the population density in 2010 in each state? How much has population density changed between 2010 and 2015 in each state? Calculate the change in percentages and map them. Change the columns names in us_states to lowercase. (Hint: helper functions - tolower() and colnames() may help). Using us_states and us_states_df create a new object called us_states_sel. The new object should have only two variables - median_income_15 and geometry. Change the name of the median_income_15 column to Income. Calculate the change in median income between 2010 and 2015 for each state. Bonus: what was the minimum, average and maximum median income in 2015 for each region? What is the region with the largest increase of the median income? Create a raster from scratch with nine rows and columns and a resolution of 0.5 decimal degrees (WGS84). Fill it with random numbers. Extract the values of the four corner cells. What is the most common class of our example raster grain (hint: modal())? Plot the histogram and the boxplot of the data(dem, package = &quot;RQGIS&quot;) raster. Now attach also data(ndvi, package = &quot;RQGIS&quot;). Create a raster stack using dem and ndvi, and make a pairs() plot References "],
["spatial-data-operations.html", "4 Spatial data operations Prerequisites 4.1 Introduction 4.2 Spatial operations on vector data 4.3 Spatial operations on raster data 4.4 Exercises", " 4 Spatial data operations Prerequisites This chapter requires the packages tidyverse, sf and raster. library(sf) library(tidyverse) library(raster) It also relies on spData, which loads datasets used in examples in the chapter: library(spData) 4.1 Introduction Spatial operations are a vital part of geocomputation. This chapter shows how spatial objects can be modified in a multitude of ways based on their location and shape. The content clearly builds on the previous chapter because many spatial operations have a non-spatial (attribute) equivalent. Spatial operations on vector objects include spatial subsetting (covered in section 4.2.1), joining and aggregation (section 4.2.3). These topics may sound daunting, but they have already been covered, in section 3.2. Spatial operations on rasters include merging and subsetting, covered in section 4.3. The chapter also introduces new concepts that are unique to spatial data. A variety of topological relations can be used to subset/join vector geometries, a topic that is explored in section 4.2.2. New geometry data can be created by modifying existing spatial objects, using operations such as ‘buffer’ and ‘clip’, described in section 4.2.6. Spatial operations on raster datasets involve map algebra (covered in sections 4.3.2 to 4.3.6) and combining and aligning them (covered in sections 4.3.7 and 4.3.8). Another unique aspect of spatial objects is distance. All spatial objects are related through space and distance calculations can be used to find the strength of this relationship between spatial entities. Distance operations are covered in sections 4.2.7 and 4.3.6 for vector and raster datasets respectively. It is important to note that spatial operations that use two spatial objects rely on both objects having the same coordinate reference system, a topic that was introduced in 2.3 and which will be covered in more depth in Chapter 6. 4.2 Spatial operations on vector data This section provides an overview of spatial operations on vector geographic data represented as simple features in the sf package before section 4.3, which presents spatial methods using the raster package. 4.2.1 Spatial subsetting Spatial subsetting is the process of selecting features of a spatial object based on whether or not they in some way relate in space to another object. It is analogous to attribute subsetting (covered in section 3.2.1) and can be done with the base R square bracket ([) operator or with the filter() function from the tidyverse. An example of spatial subsetting is provided by the nz and nz_height datasets in spData. These contain projected data on the 16 main regions and 101 highest points in New Zealand respectively (Figure 4.1). The following code chunk first creates an object representing Canterbury, then uses spatial subsetting to return all high points in the region: canterbury = nz %&gt;% filter(REGC2017_NAME == &quot;Canterbury Region&quot;) canterbury_height = nz_height[canterbury, ] Figure 4.1: Illustration of spatial subsetting with red triangles representing height points in New Zealand. The right-hand map contains only points in the Canterbury region (highlighted in grey). The points were subset with nz_height[canterbury, ]. Like attribute subsetting x[y, ] subsets features of target object x using the contents of a source object y. With spatial subsetting however, instead of y being logical — a vector of TRUE and FALSE values — it is another spatial (sf) object. Various topological relations can be used for spatial subsetting. These determine the type of spatial relationship that features in the target object must have with the subsetting object to be selected, including touches, crosses or within (see section 4.2.2). Intersects is the default spatial subsetting operator, a default that returns TRUE for many types of spatial relations, including touches, crosses and is within. These alternative spatial operators can be specified with the op = argument, as illustrated below (try plotting the result or skip to section 4.2.2 to find out what this does): nz_height[canterbury, , op = st_disjoint] Interested readers can see this default value of op set in the first line of the function call by entering its long-form name into the console sf:::`[.sf`. The ?sf help page documents this also. Another way of doing spatial subsetting relies on the ability of topological operators to return logical objects by setting the sparse argument to FALSE: sel = st_intersects(x = nz_height, y = canterbury, sparse = FALSE) canterbury_height2 = nz_height[sel, ] The preceding code chunk created an intermediary object named sel, short for selection, containing only TRUE and FALSE values. This ‘selection object’ also works with filter(), introduced in section 3.2.1: canterbury_height3 = nz_height %&gt;% filter(sel) For many applications this is all you’ll need to know about spatial subsetting and if this is the case, you can safely skip to the next section (4.2.2). If you’re interested in the details — specifically the differences between canterbury_height3 and the other canterbury_height objects, and the nature of sel — read-on. At this point there are three versions of canterbury_height, one created with spatial subsetting directly and the other two via the intermediary object sel. We can test whether they are identical as follows: identical(x = canterbury_height, y = canterbury_height2) #&gt; [1] TRUE identical(x = canterbury_height, y = canterbury_height3) #&gt; [1] FALSE What is different about canterbury_height3? The only difference is that filter() changed the row names: row.names(canterbury_height)[1:3] #&gt; [1] &quot;5&quot; &quot;6&quot; &quot;7&quot; row.names(canterbury_height3)[1:3] #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; If the row names are re-set, the objects become identical: attr(canterbury_height3, &quot;row.names&quot;) = attr(x = canterbury_height, &quot;row.names&quot;) identical(canterbury_height, canterbury_height3) #&gt; [1] TRUE This discarding of row names is not something that is specific to spatial data, as illustrated in the code chunk below. dplyr discards row names by design. For further discussion of this decision, and some controversy, see the (closed) issue #366 in the package’s issue tracker. But what is sel? It is not, as one might imagine, a logical vector. It is a logical two-dimensional object, a matrix. sel has one row per feature in the target object (nz_height) and a column per feature in the subsetting object (canterbury). Cell sel[i, j] is TRUE if the ith feature in the target object intersects with the jth feature in the subsetting object. If there is more than 1 feature in y the resulting selection matrix must be converted into a vector before it is used for subsetting, as illustrated below: co = nz %&gt;% filter(grepl(&quot;Cant|Otag&quot;, REGC2017_NAME)) sel_matrix = st_intersects(nz_height, co, sparse = FALSE) sel_vector = rowSums(sel_matrix) &gt; 0 heights_co = nz_height[sel_vector, ] The above code chunk results in an object, heights_co, that represent the high points that intersect with either Canterbury or Otago region (hence the object name co). It did this in four stages: Subset the regions of nz containing “Cant” or “Otago” in their names. This was done using the pattern matching function grepl() in combination with the | character, which means ‘or’, resulting in the subsetting object co. Create a selection matrix representing which features of nz_height intersect with the regions in co. Convert the selection matrix into a logical ‘selection vector’ by using rowSums() to find which features matched any features in co. Use the result to subset nz_heights, creating a new object heights_co. The nature of sel_matrix is verified below: class(sel_matrix) #&gt; [1] &quot;matrix&quot; typeof(sel_matrix) #&gt; [1] &quot;logical&quot; dim(sel_matrix) #&gt; [1] 101 2 4.2.2 Topological relations Topological relations define the spatial relationships between objects. To understand them, it helps to have some simple test data to work with. Figure 4.2 contains a polygon (a), a line (l) and some points (p), which are created in the code below. a_poly = st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1)))) a = st_sfc(a_poly) l_line = st_linestring(x = matrix(c(-1, -1, -0.5, 1), , 2)) l = st_sfc(l_line) p_matrix = matrix(c(0.5, 1, -1, 0, 0, 1, 0.5, 1), ncol = 2) p_multi = st_multipoint(x = p_matrix) p = st_sf(st_cast(st_sfc(p_multi), &quot;POINT&quot;)) Figure 4.2: Points (p 1 to 4), line and polygon objects arranged to demonstrate spatial relations. A simple query is: which of the points in p intersect in some way with polygon a? The question can be answered by inspection (points 1 and 2 are over or touch the triangle). It can also be answered by using the topological relation intersects, implemented in sf as follows: st_intersects(p, a) #&gt; Sparse geometry binary predicate list of length 4, where the predicate was `intersects&#39; #&gt; 1: 1 #&gt; 2: 1 #&gt; 3: (empty) #&gt; 4: (empty) The contents of the result should be as you expected: the function returns a positive (1) result for the first two points, and a negative result (represented by an empty vector) for the last two. What may be unexpected is that the result comes in the form of a list of vectors. This sparse matrix output only registers a relation if one exists, reducing the memory requirements of topological operations on multi-feature objects. As we saw in the previous section a dense matrix consisting of TRUE or FALSE values for each combination of features can also be returned when sparse = FALSE: st_intersects(p, a, sparse = FALSE) #&gt; [,1] #&gt; [1,] TRUE #&gt; [2,] TRUE #&gt; [3,] FALSE #&gt; [4,] FALSE The output is a matrix in which each row represents a feature in the target object and each column represents a feature in in the selecting object. In this case only the first two features in p intersect with a and there is only one feature in a so the result has only one column. The result can be used for subsetting as we saw in section 4.2.1. Note that st_intersects() returns TRUE for the second feature in the object p even though it just touches the polygon a: intersects is a ‘catch-all’ topological operation which identifies many types of spatial relation. The opposite of st_intersects() is st_disjoint(), which returns only objects that do not spatially relate in any way to the selecting object ([, 1] ensures the output is a vector consuming one rather than four lines when printed): st_disjoint(p, a, sparse = FALSE)[, 1] #&gt; [1] FALSE FALSE TRUE TRUE st_within() returns TRUE only for objects that are completely within the selecting object. This applies only to the second object, which is inside the triangular polygon, as illustrated below: st_within(p, a, sparse = FALSE)[, 1] #&gt; [1] TRUE FALSE FALSE FALSE Note that although the first point is within the triangle, it does not touch any part of its border. For this reason st_touches() only returns TRUE for the second point: st_touches(p, a, sparse = FALSE)[, 1] #&gt; [1] FALSE TRUE FALSE FALSE What about features that do not touch, but almost touch the selection object? These can be selected using st_is_within_distance(), which has an additional dist argument. It can be used to set how close target object need to be before they are selected. Note that although point 4 is one unit of distance from the nearest node of a (at point 2 in Figure 4.2), it is still selected when the distance is set to 0.9. This is illustrated in the code chunk below, the second line of which converts the lengthy list output into a logical object: sel = st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix lengths(sel) &gt; 0 #&gt; [1] TRUE TRUE FALSE TRUE 4.2.3 Spatial joining and aggregation Joining two non-spatial datasets relies on a shared ‘key’ variable, as described in section 3.2.3. Spatial data joining applies the same concept, but instead relies on shared areas of geographic space. As with attribute data joining adds a new column to the target object (the argument x in joining functions) from a source object (y). The process is illustrated in Figure 4.3, which shows a target object (the asia dataset, left) being joined to a source dataset (the three most populous cities of the world), resulting in a new attribute being added to the joined dataset (right). asia = world %&gt;% filter(continent == &quot;Asia&quot;) urb = urban_agglomerations %&gt;% filter(year == 2020) %&gt;% top_n(n = 3, wt = population_millions) joined = st_join(x = asia, y = urb) %&gt;% na.omit() Figure 4.3: Illustration of a spatial join: the populations of the world’s 3 largest agglomerations joined onto their respective countries. This operation is also know as spatial overlay. By default, st_join() performs a left join (see section 3.2.3), but it can also do inner joins by setting the argument left = FALSE. Like spatial subsetting, the default topological operator used by st_join() is st_intersects(). This can be changed with the join argument (see ?st_join for details). In the example above, we have added features of a point layer to a polygon layer but there might be multiple point matches per polygon. Had we chosen to select the four (instead of three) most populous cities in the world, two of them would have belonged to China (Shanghai and Beijing, give it a try yourself). In such a case st_join() simply adds a new row. In our example we would have ended up with two polygons representing China. 4.2.4 Non-overlapping joins Sometimes two geographic datasets do not touch but still have a strong geographic relationship enabling joins. The datasets cycle_hire and cycle_hire_osm, already loaded in the spData package, provide a good example. Plotting them shows that they are often closely related but they do not touch, as shown in Figure 4.4, a base version of which is created with the following code below: plot(cycle_hire$geometry, col = &quot;blue&quot;) plot(cycle_hire_osm$geometry, add = TRUE, pch = 3, col = &quot;red&quot;) We can check if any points are the same st_intersects() as show below: any(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE)) #&gt; [1] FALSE Figure 4.4: The spatial distribution of cycle hire points in London based on official data (blue) and OpenStreetMap data (red). Imagine that we need to join the capacity variable in cycle_hire_osm onto the official ‘target’ data contained in cycle_hire. This is when a non-overlapping join is needed. The simplest method is to use the topological operator st_within_distance() shown in section 4.2.2, using a treshold distance of 20 m. Note that before performing the relation both datasets must be transformed into a projected CRS, saved as new objects denoted by the affix P (for projected) below: cycle_hire_P = st_transform(cycle_hire, 27700) cycle_hire_osm_P = st_transform(cycle_hire_osm, 27700) sel = st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20) summary(lengths(sel) &gt; 0) #&gt; Mode FALSE TRUE #&gt; logical 304 438 This shows that there are 438 points in the target object cycle_hire_P within the threshold distance of cycle_hire_osm_P. How to retrieve the values associated with the respective cycle_hire_osm_P points? The solution is again with st_join() although the additional dist argument must be specified: z = st_join(cycle_hire_P, cycle_hire_osm_P, st_is_within_distance, dist = 20) nrow(cycle_hire) #&gt; [1] 742 nrow(z) #&gt; [1] 762 Note that the number of rows in the joined result is greater than the target. This is because some cycle hire stations in cycle_hire_P have multiple matches in cycle_hire_osm_P. To aggregate the values for the overlapping points and return the mean, we can use the aggregation methods learned in Chapter 3, resulting in an object with the same number of rows as the target: z = z %&gt;% group_by(id) %&gt;% summarize(capacity = mean(capacity)) nrow(z) == nrow(cycle_hire) #&gt; [1] TRUE The capacity of nearby stations can be verified by comparing a plot of the capacity of the source cycle_hire_osm data with the results in this new object (plots not show): plot(cycle_hire_osm[&quot;capacity&quot;]) plot(z[&quot;capacity&quot;]) The result of this join has used a spatial operation to change the attribute data associated with simple features but the geometry associated with each feature has remained unchanged. In the subsequent sections, we will present spatial operations that also act on and modify the underlying geometry, namely dissolving, aggregating and clipping operations. 4.2.5 Dissolving and aggregating polygons Like attribute data aggregation, covered in section 3.2.2, spatial data aggregation (also known as dissolving polygons) can be a way of condensing data. Aggregated data show some statistic about a variable (typically average or total) in relation to some kind of grouping variable. For attribute data aggregation the grouping variable is another variable, typically one with few unique values relative to the number of rows. The REGION variable in the us_states dataset is a good example: there are only four subregions but 49 states (excluding Hawaii and Alaska). In section 3.2.2 we have already seen how attribute aggregation process condensed the world dataset down into only eight rows. Spatial data aggregation is the same conceptually but in addition to aggregating the attribute data, it dissolves the underlying polygons. Here, we want to aggregate the state population into regions. This means that we not only end up with four rows but also with four polygons (out of 49 in the beginning). As with spatial subsetting, spatial aggregation operations work by extending existing functions, as illustrated in the code chunk below which aggregates US states to create the bottom map in Figure 4.5. regions = aggregate(x = us_states[, &quot;total_pop_15&quot;], by = list(us_states$REGION), FUN = sum, na.rm = TRUE) Figure 4.5: Spatial aggregation on contiguous polygons, illustrated by aggregating the population of US states into regions. Note the operation automatically dissolves boundaries between touching polygons in the same region. Spatial aggregation can also be done in the tidyverse, using dplyr functions as follows: group_by(us_states, REGION) %&gt;% summarize(sum(pop = total_pop_15, na.rm = TRUE)) # buff_agg = aggregate(x = africa[, &quot;pop&quot;], by = buff, FUN = sum) The result, buff_agg, is a spatial object with the same geometry as by (the circular buffer in this case) but with an additional variable, pop reporting summary statistics for all features in x that intersect with by (the total population of the countries that touch the buffer in this case). Plotting the result (with plot(buff_agg)) shows that the operation does not really make sense: Figure ?? shows a population of over half a billion people mostly located in a giant circle floating off the west coast of Africa! The results of the spatial aggregation exercise presented in Figure ?? are unrealistic for three reasons: People do not live in the sea (the geometry of the aggregating object is not appropriate for the geometry target object). This method would ‘double count’ countries whose borders cross aggregating polygons when multiple, spatially contiguous, features are used as the aggregating object. It is wrong to assume that all the people living in countries that touch the buffer reside within it (the default spatial operator st_intersects() is too ‘greedy’). The most extreme example of this is Algeria, the most northerly country selected: the spatial aggregation operation assumes that all 39 million Algerian citizens reside in the tiny southerly tip that is within the circular buffer. A number of methods can be used to overcome these issues, and generate a more realistic population attributed to the circular buffer illustrated in Figure ??. The simplest of these is to convert the country polygons into points representing their geographic centroids before aggregation, covered in section ??. This would ensure that any spatially contiguous aggregating object covering the target object (the Earth in this case) would result in the same total: there would be no double counting. The estimated total population residing within the study area would be more realistic if geographic centroids were used. (The centroid of Algeria, for example, is far outside the aggregating buffer.) Except in cases where the number of target features per aggregating feature is very large, or where the aggregating object is spatially congruent with the target (covered in section 4.2.5.1), using centroids can also lead to errors due to boundary effects: imagine a buffer that covers a large area but contains no centroids. These issues can be tackled when aggregating areal target data with areal interpolation. 4.2.5.1 Spatial congruence and areal interpolation Spatial congruence is an important concept related to spatial aggregation. An aggregating object object (which we will refer to as y, representing the buffer object in the previous section) is congruent with the target object (x, representing the countries in the previous section) if the two objects have shared borders. Often this is the case for administrative boundary data, whereby the larger units (e.g., Middle Layer Super Output Areas in the UK or districts in many other European countries) are composed of many smaller units (Output Areas in this case, see ons.gov.uk for further details or municipalities in many other European countries). Incongruent aggregating objects, by contrast, do not share common borders with the target (Qiu, Zhang, and Zhou 2012). This is problematic for spatial aggregation (and other spatial operations) illustrated in Figure 4.6. Areal interpolation can help to alleviate this issue. It helps to transfer data from one set of areal units to another. A number of algorithms have been developed for areal interpolation, including area weighted and pycnophylactic interpolation methods task (Tobler 1979). Figure 4.6: Illustration of congruent (left) and incongruent (right) areal units. The simplest useful method for spatial interpolation is area weighted spatial interpolation. This is implemented in st_interpolate_aw(), as demonstrated below: # buff_agg_aw = st_interpolate_aw(x = africa[&quot;pop&quot;], to = buff, extensive = TRUE) Instead of simply aggregating, this procedure additionally applies a weight, in this case simply the area. For instance, if the intersection of our buffer and a country is 100 000 km2 but the country has 1 mio square kilometers and 1 mio inhabitants, our result will obtain just a tenth of total population, in this case 100 000 inhabitants. 4.2.6 Clipping Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features. Clipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a center point one unit away from each other and radius of one: b = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points b = st_buffer(b, dist = 1) # convert points to circles l = c(&quot;x&quot;, &quot;y&quot;) plot(b) text(x = c(-0.5, 1.5), y = 1, labels = l) # add text Figure 4.7: Overlapping circles. Imagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the function st_intersection(), illustrated using objects named x and y which represent the left and right-hand circles: x = b[1] y = b[2] x_and_y = st_intersection(x, y) plot(b) plot(x_and_y, col = &quot;lightgrey&quot;, add = TRUE) # color intersecting area The subsequent code chunk demonstrate how this works for all combinations of the ‘Venn’ diagram representing x and y, inspired by Figure 5.1 of the book R for Data Science (Grolemund and Wickham 2016). Figure 4.8: Spatial equivalents of logical operators. To illustrate the relationship between subsetting and clipping spatial data, we will subset points that cover the bounding box of the circles x and y in Figure 4.8. Some points will be inside just one circle, some will be inside both and some will be inside neither. There are two different ways to subset points that fit into combinations of the circles: via clipping and logical operators. But first we must generate some points. We will use the simple random sampling strategy to sample from a box representing the extent of x and y. To generate this points will use a function not yet covered in this book, st_sample(). Next we will generate the situation plotted in Figure 4.9: bb = st_bbox(st_union(x, y)) pmat = matrix(c(bb[c(1, 2, 3, 2, 3, 4, 1, 4, 1, 2)]), ncol = 2, byrow = TRUE) box = st_polygon(list(pmat)) set.seed(2017) p = st_sample(x = box, size = 10) plot(box) plot(x, add = TRUE) plot(y, add = TRUE) plot(p, add = TRUE) text(x = c(-0.5, 1.5), y = 1, labels = l) Figure 4.9: Randomly distributed points within the bounding box enclosing circles x and y. 4.2.7 Distance relations While topological relations are binary — a feature either intersects with another or does not — distance relations are continuous. The distance between two objects is calculated with the st_distance() function. This is illustrated in the code chunk below, which finds the distance between the highest point in New Zealand and the geographic centroid of the Canterbury region, created in section 4.2.1: nz_heighest = nz_height %&gt;% top_n(n = 1, wt = elevation) canterbury_centroid = st_centroid(canterbury) st_distance(nz_heighest, canterbury_centroid) #&gt; Units: m #&gt; [,1] #&gt; [1,] 115566 There are two potentially surprising things about the result: 1) it comes with a units attribute, so you know that it’s just over 100,000 m (not 100,000 inches, or any other measure of distance!); and 2) it is returned as a matrix, even though the result only contains a single value. This second feature hints at another useful feature of st_distance(), its ability to return distance matrices between all combinations of features in objects x and y. This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co. st_distance(nz_height[1:3, ], co) #&gt; Units: m #&gt; [,1] [,2] #&gt; [1,] 123537 15498 #&gt; [2,] 94283 0 #&gt; [3,] 93019 0 Note that the distance between the second and third feature in nz_height and the second feature in co is zero. This demonstrates the fact that distances between points and polygons refer to the distance to any part of the polygon: The second and third points in nz_height are in Otago, which can be verified by plotting them (result not shown): plot(co$geometry[2]) plot(nz_height$geometry[2:3], add = TRUE) 4.3 Spatial operations on raster data This section builds on 3.3, which highlights various basic methods for manipulating raster datasets, to demonstrate more advanced and explicitly spatial raster operations, and uses the same object elev and grain. 4.3.1 Spatial subsetting In the previous chapter (section 3.3) we have already learned how to subset raster datasets using cell IDs and matrix indexing. Naturally, we can subset rasters also with the help of coordinates and spatial objects. To use coordinates for subsetting, we have to ‘translate’ them into the corresponding cell ID(s) or by using the extract() command. This operation is also known as extracting values/attributes to points. # point within the top left pixel elev[cellFromXY(elev, xy = c(-1.5, 1.5))] # the same as extract(elev, data.frame(x = -1.5, y = 1.5)) The cellFromXY() and the extract() command accept also a SpatialPoints or SpatialPointsDataFrame object (though not an sf object). We can also use a raster object to subset another raster object (Figure 4.10 left panel). clip = raster(nrow = 3, ncol = 3, res = 0.3, xmn = 0.9, xmx = 1.8, ymn = -0.45, ymx = 0.45, vals = rep(1, 9)) elev[clip] #&gt; [1] 18 24 # we can also use extract # extract(elev, extent(clip)) Basically, this amounts to retrieving the values of the first raster (here: elev) falling within the extent of a second raster (here: clip). To retrieve a spatial output, we can tell R to keep the matrix structure. This will return the two output values as a raster object. elev[clip, drop = FALSE] #&gt; class : RasterLayer #&gt; dimensions : 2, 1, 2 (nrow, ncol, ncell) #&gt; resolution : 0.5, 0.5 (x, y) #&gt; extent : 1, 1.5, -0.5, 0.5 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : in memory #&gt; names : layer #&gt; values : 18, 24 (min, max) For the same operation we can also use the intersect() and crop() command. Figure 4.10: Subsetting raster values with the help of another raster (left). Raster mask (middle). Output of masking a raster. Frequently, however, we have two rasters with the same extent and resolution where one raster object serves as a mask (Figure 4.10 middle and right panel). In these cases intersect() and crop() are of little use. Instead we can use the [ again or the mask() and overlay() commands: rmask = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = sample(c(FALSE, TRUE), 36, replace = TRUE)) elev[rmask, drop = FALSE] # using the mask command mask(elev, rmask, maskvalue = TRUE) # using overlay # first we replace FALSE by NA rmask[rmask == FALSE] = NA # then we retrieve the maximum values overlay(elev, rmask, fun = &quot;max&quot;) In the code chunk above, we have created a mask object called rmask randomly setting its values to FALSE and TRUE. Next we only want to keep those values of elev which are TRUE in rmask, or expressed differently, we want to mask elev with rmask. These operations are in fact Boolean local operations since we compare cell-wise two rasters. The next subsection explores these and related operations in more detail. 4.3.2 Map algebra Map algebra makes raster processing really fast. This is because raster datasets only implicitly store coordinates. To derive the coordinate of a specific cell, we have to calculate it using its matrix position and the raster resolution and origin. For the processing, however, the geographic position of a cell is barely relevant as long as we make sure that the cell position is still the same after the processing (one-to-one locational correspondence). Additionally, if two or more raster datasets share the same extent, projection and the resolution, one could treat them as matrices for the processing. This is exactly what map algebra is doing. First, it checks the headers of the rasters on which to perform any algebraic operation, and only if they correspondent to each other, the processing goes on. And secondly, map algebra retains the so-called one-to-one locational correspondence. This is where it substantially differs from matrix algebra which changes positions when for example multiplying or dividing matrices. Map algebra (or cartographic modeling) divides raster operations into four subclasses (Tomlin 1990), with each of them either working on one or several grids simultaneously: Local or per-cell operations. Focal or neighborhood operations. Most often the output cell value is the result of a 3 x 3 input cell block. Zonal operations are similar to focal operations but instead of a predefined neighborhood, classes, which can take on any, i.e., also an irregular size and shape, are the basis for calculations. Global or per-raster operations, that means the output cell derives its value potentially from one or several entire rasters This classification scheme uses basically the number of cells involved in a processing step as distinguishing feature. Raster operations can also be classified by discipline, for example terrain, hydrological analysis or image classifications. The following sections explain how each type of map algebra operations can be used, with reference to worked examples (also see vignette(&quot;Raster&quot;) for a technical description of map algebra). 4.3.3 Local operations Local operations comprise all cell-by-cell operations in one or several layers. A good example is the classification of intervals of numeric values into groups such as grouping a digital elevation model into low (class 1), middle (class 2) and high elevations (class 3). Using the reclassify() command, we need first to construct a reclassification matrix, where the first column corresponds to the lower and the second column to the upper end of the class. The third column represents the new value for the specified ranges in column one and two. Here, we assign the raster values in the ranges 0–12, 12–24 and 24–36 are reclassified to take values 1, 2 and 3, respectively. rcl = matrix(c(0, 12, 1, 12, 24, 2, 24, 36, 3), ncol = 3, byrow = TRUE) recl = reclassify(elev, rcl = rcl) Raster algebra is another classical use case of local operations. This includes adding, subtracting and squaring two rasters. Raster algebra also allows logical operations such as finding all raster cells that are greater than a specific value (5 in our example below). The raster package supports all these operations and more, as described in vignette(&quot;Raster&quot;) and demonstrated below (results not show): elev + elev elev^2 log(elev) elev &gt; 5 Instead of arithmetic operators, one can also use the calc() and overlay() functions. These functions are more efficient, hence, they are preferable in the presence of large raster datasets. Additionally, they allow to directly store an output file. The calculation of the normalized difference vegetation index (NDVI) is one of the most famous local, i.e. pixel-by-pixel, raster operations. It ranges between - 1 and 1 with positive values indicating the presence of living plants (mostly &gt; 0.2). To calculate the NDVI, one uses the red and near-infrared bands of remotely sensed imagery (e.g., Landsat or Sentinel imagery) exploiting the fact that vegetation absorbs light heavily in the visible light spectrum, and especially in the red channel, while reflecting it in the near-infrared spectrum. \\[ \\begin{split} NDVI&amp;= \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}}\\\\ \\end{split} \\] where NIR = near infrared channel Red = red channel Predictive mapping is another interesting application of local raster operations. The response variable correspond to measured or observed points in space, for example, species richness, the presence of landslides, tree disease or crop yield. Consequently, we can easily retrieve space- or airborne predictor variables from various rasters (elevation, pH, precipitation, temperature, landcover, soil class, etc.). Subsequently, we model our response as a function of our predictors using lm, glm, gam or a machine-learning technique. To make a spatial prediction, all we have to do, is to apply the estimated coefficients to the predictor rasters, and summing up the resulting output rasters (see also Muenchow et al. (2013)). 4.3.4 Focal operations While local functions operate on one cell, though possibly from multiple layers, focal operations take into account a central cell and its neighbors. The neighborhood (also named kernel, filter or moving window) under consideration is typically of size 3-by-3 cells (that is the central cell and its eight surrounding neighbors) but can take on any other (not necessarily rectangular) shape as defined by the user. A focal operation applies an aggregation function to all cells within the specified neighborhood, uses the corresponding output as the new value for the the central cell, and moves on to the next central cell (Figure 4.11). Other names for this operation are spatial filtering and convolution (Burrough, McDonnell, and Lloyd 2015). In R, we can use the focal() function to perform spatial filtering. We define the shape of the moving window with a matrix whose values correspond to weights (see w parameter in the code chunk below). Secondly, the fun parameter lets us specify the function we wish to apply to this neighborhood. Here, we choose the minimum, but any other summary function, including sum(), mean(), or var() can be used. r_focal = focal(elev, w = matrix(1, nrow = 3, ncol = 3), fun = min) Figure 4.11: Input raster (left) and resulting output raster (right) due to a focal operation - summing up 3-by-3 windows. We can quickly check if the output meets our expectations. In our example, the minimum value has to be always the upper left corner of the moving window (remember we have created the input raster by rowwise incrementing the cell values by one starting at the upper left corner). In this example, the weighting matrix consists only of 1s, meaning each cell has the same weight on the output, but this can be changed. Focal functions or filters play a dominant role in image processing. Low-pass or smoothing filters use the mean function to remove extremes. In the case of categorical data, we can replace the mean with the mode, which is the most common value. By contrast, high-pass filters accentuate features. The line detection Laplace and Sobel filters might serve as an example here. Check the focal() help page how to use them in R. Also, terrain processing uses heavily focal functions. Think, for instance, of the calculation of the slope, aspect and flow directions. The terrain() function lets you compute a few of these terrain characteristics but has not implemented all popular methods For example, the Zevenbergen and Thorne method to compute the slope is missing. Equally, many other terrain and GIS functions are not implemented in R such as curvatures, contributing areas, different wetness indexes, and many more. Fortunately, desktop GIS commonly provide these algorithms. In Chapter 13 we will learn how to access GIS functionality from within R. 4.3.5 Zonal operations Zonal operations are similar to focal operations. The difference is that zonal filters can take on any shape instead of just a predefined window. Our grain size raster is a good example (Figure 3.2) because the different grain sizes are spread in an irregular fashion throughout the raster. To find the mean elevation for each grain size class, we can use the zonal() command. This kind of operation is also known as zonal statistics in the GIS world. z = zonal(elev, grain, fun = &quot;mean&quot;) %&gt;% as.data.frame z #&gt; zone mean #&gt; 1 1 16.8 #&gt; 2 2 19.4 #&gt; 3 3 19.8 This returns the statistics for each category, here the mean altitude for each grain size class and can be added to the attribute table of the ratified raster (see previous chapter). 4.3.6 Global operations and distances Global operations are a special case of zonal operations with the entire raster dataset representing a single zone. The most common global operations are descriptive statistics for the entire raster dataset such as the minimum or maximum (see previous chapter). Aside from that, global operations are also useful for the computation of distance and weight rasters. In the first case, one can calculate the distance from each cell to a specific target cell. For example, one might want to compute the distance to the nearest coast (see also raster::distance()). We might also want to consider topography, that means, we are not only interested in the pure distance but would like also to avoid the crossing of mountain ranges when going to the coast. To do so, we can weight the distance with elevation so that each additional altitudinal meter ‘prolongs’ the euclidean distance. Visibility and viewshed computations also belong to the family of global operations . Many map algebra operations have a counterpart in vector processing (Liu and Mason 2009). Computing a distance raster (zonal operation) while only considering a maximum distance (logical focal operation) is the equivalent to a vector buffer operation (section 4.2.6). Reclassifying raster data (either local or zonal function depending on the input) is equivalent to dissolving vector data (section 4.2.3). Overlaying two rasters (local operation), where one contains NULL or NA values representing a mask, is similar to vector clipping (section 4.2.6). Quite similar to spatial clipping is intersecting two layers (section 4.2.1). The difference is that two these two layers (vector or raster) simply share an overlapping area (see Figure 4.8 for an example). However, be careful with the wording. Sometimes the same words have slightly different meanings for raster and vector data models. Aggregating in the case of vector data refers to dissolving polygons while it means increasing the resolution in the case of raster data. In fact, one could see dissolving or aggregating polygons as decreasing the resolution. However, zonal operations might be the better raster equivalent compared to changing the cell resolution. Zonal operations can dissolve the cells of one raster in accordance with the zones (categories) of another raster using an aggregation function (see above). 4.3.7 Merging rasters Suppose we would like to compute the NDVI (see section 4.3.3), and additionally want to compute terrain attributes from elevation data for observations within a study area. Before such computations we would have to acquire airborne or remotely sensed information. The corresponding imagery is often divided into scenes covering a specific spatial extent. Frequently, a study area covers more than one scene. In these cases we would like to merge the scenes covered by our study area. In the easiest case, we can just merge these scenes, that is put them side to side. This is possible with digital elevation data (SRTM, ASTER). In the following code chunk we first download the SRTM elevation data for Austria and Switzerland (for the country codes have a look at ccodes()). In a second step, we merge the two rasters into one. aut = getData(&quot;alt&quot;, country = &quot;AUT&quot;, mask = TRUE) ch = getData(&quot;alt&quot;, country = &quot;CHE&quot;, mask = TRUE) aut_ch = merge(aut, ch) Raster’s merge() command combines two images, and in case they overlap, it uses the value of the first raster. You can do exactly the same with gdalUtils::mosaic_rasters() which is faster, and therefore recommended if you have to merge a multitude of large rasters stored on disk. The merging approach is of little use when the overlapping values do not correspond to each other. This is frequently the case when you want to combine spectral imagery from scenes that were taken on different dates. The merge() command will still work but you will see a clear border in the resulting image. The mosaic() command lets you define a function for the overlapping area. For instance, we could compute the mean value. This might smooth the clear border in the merged result but it will most likely not make it disappear. To do so, we need a more advanced approach. Remote scientist frequently apply histogram matching or use regression techniques to align the values of the first image with those of the second image. The packages landsat (histmatch(), relnorm(), PIF()), satellite (calcHistMatch()) and RStoolbox (histMatch(), pifMatch()) provide the corresponding functions. 4.3.8 Aligning rasters When merging or performing map algebra on rasters, their resolution, projection, origin and/or extent has to match. Otherwise, how should we add the values of one raster with a resolution of 0.2 decimal degrees to a second with a resolution of 1 decimal degree? The same problem arises when we would like to merge satellite imagery from different sensors with different projections and resolutions. We can deal with such mismatches by aligning the rasters. The projectRaster() function reprojects one raster to a desired projection, say from UTM to WGS84. Equally, map algebra operations require the same extent. Following code adds one row and two columns to each side of the raster while setting all new values to an elevation of 1000 meters (4.12). elev_2 = extend(elev, c(1, 2), value = 1000) plot(elev_2) Figure 4.12: Original raster extended by 1 one row on each side (top, bottom) and two columns on each side (right, left). Performing an algebraic operation on two objects with differing extents in R, the raster package returns the result for the intersection, and says so in a warning. elev_3 = elev + elev_2 #&gt; Warning in elev + elev_2: Raster objects have different extents. Result for #&gt; their intersection is returned However, we can also align the extent of two rasters with the extend() command. Here, we extend the elev object to the extend of elev_2. The newly added rows and column receive the default value of the value parameter, i.e., NA. elev_4 = extend(elev, elev_2) The aggregate() and disaggregate() functions help to change the cell size resolution of a raster. For instance, let us aggregate elev from a resolution of 0.5 to a resolution of 1, that means we aggregate by a factor of 2 (Fig. 4.13). Additionally, the output cell value should correspond to the mean of the input cells (but one could use other functions as well such as median(), sum(), etc.): elev_agg = aggregate(elev, fact = 2, fun = mean) par(mfrow = c(1, 2)) plot(elev) plot(elev_agg) Figure 4.13: Original raster (left). Aggregated raster (right). Note that the origin of elev_agg has changed, too. origin(elev) #&gt; [1] 0 0 origin(elev_agg) #&gt; [1] 0.5 0.5 The origin is the point closest to (0, 0) if you moved towards it in steps of x and y resolution. If two rasters have different origins, their cells do not overlap completely which would make map algebra impossible. To change the origin , use origin().20 Looking at figure 4.14 reveals the effect of changing the origin. # plot the aggregated raster plot(elev_agg) # change the origin origin(elev_agg) = c(0, 0) # plot it again plot(elev_agg, add = TRUE) Figure 4.14: Plotting rasters with the same values but different origins. The resample() command lets you align several raster properties in one go, namely origin, extent and resolution. Let us resample an extended elev_agg to the properties of elev again. # add 2 rows and columns, i.e. change the extent elev_agg = extend(elev_agg, 2) elev_disagg = resample(elev_agg, elev) Though our disaggregated elev_disagg retrieved back its original resolution, cell size and extent, its values differ. However, this is to be expected, disaggregating cannot predict values at a finer resolution, it simply uses an interpolation algorithm. It is important to keep in mind that disaggregating results in a finer resolution, the corresponding values, however, are only as accurate as their lower resolution source. Finally, if you want to align many (possibly hundreds or thousands of) images stored on disk, you might want to checkout the gdalUtils::align_rasters() function. Nevertheless, you may also use raster with very large datasets. This is because raster: lets you work with raster datasets that are too large to fit into the main memory (RAM) by only processing chunks of it. tries to facilitate parallel processing. For more information have a look at the help pages of beginCluster() and clusteR(). Additionally, check out the Multi-core functions section in vignette(&quot;functions&quot;, package = &quot;raster&quot;). 4.4 Exercises Write code that subsets points that are contained within x and y (illustrated by the plot in the 2nd row and the 1st column in Figure 4.8). Create a randomly located point with the command st_point() (refer back to section 2.1.5.2 to see how to create spatial data ‘from scratch’). Write code that uses functions aggregate() and st_buffer() to answers the following question: What proportion of the world’s population lives in countries that intersect a circle with a 10 degree radius of the intersection between the equator and the 9th meridian? (Advanced challenge: find the point with the highest number of people within a 10 degree radius.) #&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs): st_buffer does #&gt; not correctly buffer longitude/latitude data #&gt; dist is assumed to be in decimal degrees (arc_degrees). #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar #&gt; [1] 0.00998 Assuming that people are evenly distributed across countries, estimate the population living within the circle created to answer the previous question. #&gt; although coordinates are longitude/latitude, st_intersection assumes that they are planar #&gt; Warning in st_interpolate_aw(x = world[&quot;pop&quot;], to = buff9, extensive = #&gt; TRUE): st_interpolate_aw assumes attributes are constant over areas of x Use data(dem, package = &quot;RQGIS&quot;), and reclassify the elevation in three classes: low, middle and high. Secondly, compute the NDVI (data(ndvi, package = &quot;RQGIS&quot;)) and the mean elevation for each altitudinal class. Apply a line detection filter to data(dem, package = &quot;RQGIS&quot;). Calculate the NDVI of a Landsat image. Use the Landsat image provided by the spDataLarge package (system.file(&quot;raster/landsat.tif&quot;, package=&quot;spDataLarge&quot;)). This post shows how to compute distances to the nearest coastline using raster::distance(). Retrieve a digital elevation model of Spain, and compute a raster which represents the distance to the coast. (Hint: Have a look at getData() to retrieve a digital elevation model and administrative boundaries for Spain.) Before, computing the distance raster, you might want to increase the resolution of the input dem raster, otherwise computing time might become too long. Secondly, weight the distance raster with elevation. Every 100 altitudinal meters should increase the distance to the coast by 10 km. Finally, compute the difference between the raster using the euclidean distance and the raster weighted by elevation. (Note that this is a very simple weighting approach. A more advanced approach might instead weight by flow direction, i.e. favor the steepest drop or the slightest increase in elevation.) References "],
["read-write.html", "5 Geographic data I/O Prerequisites 5.1 Introduction 5.2 Retrieving open data 5.3 File formats 5.4 Data Input (I) 5.5 Data output (O) 5.6 Visual outputs 5.7 Exercises", " 5 Geographic data I/O Prerequisites This chapter requires the packages tidyverse, sf, spData and raster. library(tidyverse) library(sf) library(spData) library(raster) 5.1 Introduction This chapter is about reading and writing geographic data. Geographic data import is an essential part of geocomputational software because without data real-world applications are impossible. The skills taught in this book will enable you to add value to data meaning that, for others to benefit from the results, data output is also vital. These two processes go hand-in-hand and are referred to as I/O — short for input/output — in Computer Science (Gillespie and Lovelace 2016). Hence the title of this chapter. Geographic data I/O is almost always part of a wider process. It depends on knowing which datasets are available, where they can be found and how to retrieve them, topics covered in section 5.2. This section demonstrates how to access open access geoportals which collectively contain many terrabytes of data. There is a wide range of geographic file formats, each of which has pros and cons. These are described in section 5.3. The process of actually reading and writing such file formats efficiently is not covered until sections 5.4 and 5.5 respectively. The final section (5.6) demonstrates methods for saving visual outputs (maps), in preparation for a subsequent chapter dedicated to visualization. 5.2 Retrieving open data Nowadays, a vast amount of spatial data is available on the internet. Best of all, much of it is freely available. You just have to know where to look. While we cannot provide a comprehensive guide to all available geodata, we point to a few of the most important sources. Various ‘geoportals’ (web services providing geographic data such as the geospatial datasets in Data.gov) are a good place to start, providing a wide range of geographic data. Geoportals are a very useful data source but often only contain data for a specific locations (see the Wikipedia page on geoportals for a list of geoportals covering many areas of the world). To overcome this limitation some global geoportals have been developed. The GEOSS portal, for example, provides global remote sensing data. Additional geoportals with global coverage and an emphasis on raster data include the EarthExplorer and the Copernicus Open Access Hub. A wealth of European data is available from the INSPIRE geoportal. Typically, geoportals provide an interface that lets you query interactively the existing data (spatial and temporal extent, and product). However, in this book we encourage you to create reproducible workflows. In many cases data downloads can be scripted via download calls to static URLs or APIs (see the Sentinel API for example), saving time, and enabling others to repeat and update the unglamorous data download process. Traditionally, files have been stored on servers. You can easily download such files with the download.file() command. For example, to download National Park Service units in the United States, run: url = file.path(&quot;http://www.naturalearthdata.com/http//www.naturalearthdata.com&quot;, &quot;download/10m/cultural/ne_10m_parks_and_protected_lands.zip&quot;) download.file(url = url, destfile = &quot;USA_parks.zip&quot;) unzip(zipfile = &quot;USA_parks.zip&quot;) usa_parks = st_read(&quot;ne_10m_parks_and_protected_lands_area.shp&quot;) Specific R packages providing an interface to spatial libraries or geoportals are even more user-friendly (Table 5.1). Table 5.1: Selected R packages for spatial data retrieval Package name Description osmdata Download and import of OpenStreetMap data. raster The getData() function downloads and imports administrative country, SRTM/ASTER elevation, WorldClim data. rnaturalearth Functions to download Natural Earth vector and raster data, including world country borders. rnoaa An R interface to National Oceanic and Atmospheric Administration (NOAA) climate data. rWBclimate An access to the World Bank climate data. For example, you can get the borders of any country with the ne_countries() function from the rnaturalearth package: library(rnaturalearth) usa = ne_countries(country = &quot;United States of America&quot;) # United States borders class(usa) #&gt; [1] &quot;SpatialPolygonsDataFrame&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;sp&quot; # you can do the same with raster::getData() # getData(&quot;GADM&quot;, country = &quot;USA&quot;, level = 0) As a default, rnaturalearth returns the output as a Spatial* class. You can easily convert it to the sf class with st_as_sf(): usa_sf = st_as_sf(usa) As a second example, we will download a raster dataset. The code below downloads a series of rasters that contains global monthly precipitation sums. The spatial resoultion is ten minutes. The result is a multilayer object of class RasterStack. library(raster) worldclim_prec = getData(name = &quot;worldclim&quot;, var = &quot;prec&quot;, res = 10) class(worldclim_prec) #&gt; [1] &quot;RasterStack&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;raster&quot; A third example uses the recently developed package osmdata (Padgham et al. 2017) to find parks from the OpenStreetMap (OSM) database. As illustrated in the code-chunk below, queries begin with the function opq() (short for OpenStreetMap query), the first argument of which is bounding box, or text string representing a bounding box (the city of Leeds in this case). The result is passed to a function for selecting which OSM elements we’re interested in (parks in this case), represented by key-value pairs, which in turn is passed to the function osmdata_sf() which does the work of downloading the data and converting it into a list of sf objects (see vignette('osmdata') for further details): library(osmdata) parks = opq(bbox = &quot;leeds uk&quot;) %&gt;% add_osm_feature(key = &quot;leisure&quot;, value = &quot;park&quot;) %&gt;% osmdata_sf() OpenStreetMap is a vast global database of crowd-sourced data and it is growing by the minute. Although the quality is not as spatially consistent as many official datasets, OSM data have many advantages: they are globally available free of charge and using crowd-source data can encourage ‘citizen science’ and contributions back to the digital commons. Finally, R packages might contain or just consist of spatial data (e.g., package spData). You can access such data with the data() function. For example, you can get a list of dataset in a package, data(package = &quot;spData&quot;). To attach the dataset to the global environment specify the name of a dataset (data(&quot;cycle_hire&quot;, package = &quot;spData&quot;)). Sometimes, packages come also with the original files.21. To load such a file from the package, you need to specify the package name and the relative path to the dataset, for example: world_raw_filepath = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) world_raw = st_read(world_raw_filepath) #&gt; Reading layer `wrld.gpkg&#39; from data source `/home/travis/R/Library/spData/shapes/world.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs Find more information on getting data using R packages in section 5.5 and section 5.6 of Gillespie and Lovelace (2016). 5.3 File formats Spatial data is usually stored as files or in spatial databases. File-based data formats can contain either vector or raster data, while spatial databases could store both. Historically, GIS file formats were developed by mapping agencies and software companies. Exchanging spatial data between different software packages each coming with its own format was troublesome in the beginning. GDAL22 put an end to these troubles by enabling reading and writing many raster and vector data formats. Subsequently, many open and proprietary GIS software (e.g., GRASS, QGIS, ArcGIS, ENVI) were quick to incorporate it. Another change for spatial data formats came with the foundation of the Open Geospatial Consortium (OGC)23. This organization collaborates on the development and implementation of open standards for geospatial content including file formats such as the KML and GeoPackage formats as well as the simple feature standard. Developing and maintaining spatial file formats in an open model provides several benefits over the proprietary formats , and eases interoperability. Nowadays, more than a hundred spatial data formats exist. Table 5.2 presents some basic information about selected, often used spatial file formats. Table 5.2: Selected spatial file formats. Name Extension Info Type Model ESRI Shapefile .shp (the main file) One of the most popular vector file format. Consists of at least three files. The main files size cannot exceed 2 GB. It lacks support for mixed type. Columns names are limited to 10 characters, and number of columns are limited at 255. It has poor support for Unicode standard. Vector Partially open GeoJSON .geojson Extends the JSON exchange format by including a subset of the simple feature representation. Vector Open KML .kml XML-based format for spatial visualization, developed for use with Google Earth. Zipped KML file forms the KMZ format. Vector Open GPX .gpx XML schema created for exchange of GPS data. Vector Open GeoTIFF .tiff GeoTIFF is one of the most popular raster formats. Its structure is similar to the regular .tif format, however, additionally stores the raster header. Raster Open Arc ASCII .asc Text format where the first six lines represent the raster header, followed by the raster cell values arranged in rows and columns. Raster Open R-raster .gri, .grd Native raster format of the R-package raster. Raster Open SQLite/SpatiaLite .sqlite SQLite is a standalone, relational database management system. It is used as a default database driver in GRASS GIS 7. SpatiaLite is the spatial extension of SQLite providing support for simple features. Vector and raster Open ESRI FileGDB .gdb Collection of spatial and nonspatial objects created in the ArcGIS software. It allows to store multiple feature classes and enables use of topological definitions. Limited access to this format is provided by GDAL with the use of the OpenFileGDB and FileGDB drivers. Vector and raster Proprietary GeoPackage .gpkg Lightweight database container based on SQLite allowing an easy and platform-independent exchange of geodata Vector and raster Open 5.4 Data Input (I) Executing commands such as sf::st_read() (the main function we use for loading vector data) or raster::raster() (the main function used for loading raster data) silently sets off a chain of events that reads data from files. Moreover, there are many R packages containing a wide range of spatial data or providing simple access to different data sources. All of them load the data into R or, more precisely, assign objects to your workspace, stored in RAM accessible from the .GlobalEnv24 of your current R session. 5.4.1 Vector data Spatial vector data comes in a wide variety of file formats, most of which can be read-in via the sf function st_read(). Behind the scenes this calls GDAL. To find out which data formats sf supports, run st_drivers(). Here, we show only the first five drivers (see Table 5.3): sf_drivers = st_drivers() head(sf_drivers, n = 5) Table 5.3: Sample of available drivers for reading/writing vector data (it could vary between different GDAL versions). name long_name write copy is_raster is_vector ESRI Shapefile ESRI Shapefile TRUE FALSE FALSE TRUE GPX GPX TRUE FALSE FALSE TRUE KML Keyhole Markup Language (KML) TRUE FALSE FALSE TRUE GeoJSON GeoJSON TRUE FALSE FALSE TRUE GPKG GeoPackage TRUE TRUE TRUE TRUE The first argument of st_read() is dsn, which should be a text string or an object containing a single text string. The content of a text string could vary between different drivers. In most cases, as with the ESRI Shapefile (.shp) or the GeoPackage format (.gpkg), the dsn would be a file name. st_read() guesses the driver based on the file extension, as illustrated for a .gpkg file below: vector_filepath = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) world = st_read(vector_filepath) #&gt; Reading layer `wrld.gpkg&#39; from data source `/home/travis/R/Library/spData/shapes/world.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs For some drivers, dsn could be provided as a folder name, access credentials for a database, or a GeoJSON string representation (see the examples of the st_read() help page for more details). Some vector driver formats can store multiple data layers. By default, st_read automatically reads the first layer of the file specified in dsn, however, using the layer argument you can specify any other layer. Naturally, some options are specific to certain drivers25. For example, think of coordinates stored in a spreadsheet format (.csv). To read in such files as spatial objects, we naturally have to specify the names of the columns (X and Y in our example below) representing the coordinates. We can do this with the help of the options parameter. To find out about possible options, please refer to the ‘Open Options’ section of the corresponding GDAL driver description. For the comma-separated value (csv) format, visit http://www.gdal.org/drv_csv.html. cycle_hire_txt = system.file(&quot;misc/cycle_hire_xy.csv&quot;, package = &quot;spData&quot;) cycle_hire_xy = st_read(cycle_hire_txt, options = c(&quot;X_POSSIBLE_NAMES=X&quot;, &quot;Y_POSSIBLE_NAMES=Y&quot;)) Instead of columns describing xy-coordinates, a single column can also contain the geometry information. Well-known text (WKT), well-known binary (WKB), and the GeoJSON formats are examples for this. For instance, the world_wkt.csv file has a column named WKT representing polygons of the world’s countries. We will again use the options parameter to indicate this. Here, we will use read_sf() which does exactly the same as st_read() except it does not print the driver name to the console and stores strings as characters instead of factors. world_txt = system.file(&quot;misc/world_wkt.csv&quot;, package = &quot;spData&quot;) world_wkt = read_sf(world_txt, options = &quot;GEOM_POSSIBLE_NAMES=WKT&quot;) # the same as world_wkt = st_read(world_txt, options = &quot;GEOM_POSSIBLE_NAMES=WKT&quot;, quiet = TRUE, stringsAsFactors = FALSE) Not all of the supported vector file formats store information about their coordinate reference system. In these situations, it is possible to add the missing information using the st_set_crs() function. Please refer also to section 2.3 for more information. As a final example, we will show how st_read() also reads KML files. A KML file stores geographic information in XML format - a data format for the creation of web pages and the transfer of data in an application-independent way (Nolan and Lang 2014). Here, we access a KML file from the web. This file contains more than one layer. st_layers() lists all available layers. We choose the first layer Placemarks and say so with the help of the layer parameter in read_sf(). url = &quot;https://developers.google.com/kml/documentation/KML_Samples.kml&quot; st_layers(url) #&gt; Driver: LIBKML #&gt; Available layers: #&gt; layer_name geometry_type features fields #&gt; 1 Placemarks 3 11 #&gt; 2 Styles and Markup 1 11 #&gt; 3 Highlighted Icon 1 11 #&gt; 4 Ground Overlays 1 11 #&gt; 5 Screen Overlays 0 11 #&gt; 6 Paths 6 11 #&gt; 7 Polygons 0 11 #&gt; 8 Google Campus 4 11 #&gt; 9 Extruded Polygon 1 11 #&gt; 10 Absolute and Relative 4 11 kml = read_sf(url, layer = &quot;Placemarks&quot;) 5.4.2 Raster data Similar to vector data, raster data comes in many file formats with some of them supporting even multilayer files. raster’s raster() command reads in a single layer. raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) single_layer = raster(raster_filepath) In case you want to read in a single band from a multilayer file use the band parameter to indicate a specific layer. multilayer_filepath = system.file(&quot;raster/landsat.tif&quot;, package = &quot;spDataLarge&quot;) band3 = raster(multilayer_filepath, band = 3) If you want to read in all bands, use brick() or stack(). multilayer_brick = brick(multilayer_filepath) multilayer_stack = stack(multilayer_filepath) Please refer to section 2.2.3 for information on the difference between raster stacks and bricks. 5.5 Data output (O) The ability of writing spatial data could be used for conversion between different formats and for saving newly created objects. Depending on the data type (vector or raster), object class (e.g multipoint or RasterLayer), and type and amount of stored information (e.g. object size, range of values) - it is important to know how to store spatial files in the most efficient way. The next two section will show how to do that. 5.5.1 Vector data The counterpart of st_read() is st_write(). It allows to write sf objects to a wide range of geographic vector file formats, including the most common ones such as .geojson, .shp and .gpkg. Based on the file name, st_write() decides automatically which driver to use. How fast the writing process is depends also on the driver. st_write(obj = world, dsn = &quot;world.gpkg&quot;) #&gt; Writing layer `world&#39; to data source `world.gpkg&#39; using driver `GPKG&#39; #&gt; features: 177 #&gt; fields: 10 #&gt; geometry type: Multi Polygon Note: if you try to write to the same data source again, the function will fail: st_write(obj = world, dsn = &quot;world.gpkg&quot;) #&gt; Updating layer `world&#39; to data source `/home/travis/build/Robinlovelace/geocompr/world.gpkg&#39; using driver `GPKG&#39; #&gt; Warning in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : GDAL Error 1: Layer world already exists, CreateLayer failed. #&gt; Use the layer creation option OVERWRITE=YES to replace it. #&gt; Creating layer world failed. #&gt; Error in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : Layer creation failed. The error message provides some information as to why the function failed. The GDAL Error 1 statement makes clear that the failure occurred at the GDAL level. Additionally, the suggestion to use OVERWRITE=YES provides a clue how to fix the problem. However, this is not a st_write() argument, it is a GDAL option. Luckily, st_write provides a layer_options argument through which we can pass driver-dependent options: st_write(obj = world, dsn = &quot;world.gpkg&quot;, layer_options = &quot;OVERWRITE=YES&quot;) Another solution is to use the st_write() argument delete_layer. Setting it to TRUE deletes already existing layers in the data source before the function attempts to write (note there is also a delete_dsn argument): st_write(obj = world, dsn = &quot;world.gpkg&quot;, delete_layer = TRUE) You can achieve the same with write_sf() since it is equivalent to (technically an alias for) st_write(), except that its defaults for delete_layer and quiet is TRUE. write_sf(obj = world, dsn = &quot;world.gpkg&quot;) The layer_options argument could be also used for many different purposes. One of them is to write spatial data to a text file. This can be done by specifying GEOMETRY inside of layer_options. It could be either AS_XY for simple point datasets (it creates two new columns for coordinates) or AS_WKT for more complex spatial data (one new column is created which contains the well-known-text representation of spatial objects). st_write(cycle_hire_xy, &quot;cycle_hire_xy.csv&quot;, layer_options = &quot;GEOMETRY=AS_XY&quot;) st_write(world_wkt, &quot;world_wkt.csv&quot;, layer_options = &quot;GEOMETRY=AS_WKT&quot;) 5.5.2 Raster data The writeRaster() function saves Raster* objects to files on disk. The function expects input regarding output datatype, file format, but also accepts GDAL options specific to a selected file format (see ?writeRaster for more details). The raster package offers nine datatypes when saving a raster: LOG1S, INT1S, INT1U, INT2S, INT2U, INT4S, INT4U, FLT4S, and FLT8S26. The datatype determines the bit representation of the raster object written to disk (5.4). Which datatype to use depends on the range of the values of your raster object. The more values a datatype can represent, the larger the file will get on disk. Commonly, one would use LOG1S for bitmap (binary) rasters. Unsigned integers (INT1U, INT2U, INT4U) are suitable for categorical data, while float numbers (FLT4S and FLTS8S) usually represent continuous data. writeRaster() uses FLT4S as the default. While this works in most cases, the size of the output file will be unnecessarly large if you save binary or categorical data. Therefore, we would recommend to use the datatype that needs the least storing space but is still able to represent all values (check the range of values with the summary() function). Table 5.4: Datatypes supported by the raster package Datatype Minimum value Maximum value LOG1S FALSE (0) TRUE (1) INT1S -127 127 INT1U 0 255 INT2S -32,767 32,767 INT2U 0 65,534 INT4S -2,147,483,647 2,147,483,647 INT4U 0 4,294,967,296 FLT4S -3.4e+38 3.4e+38 FLT8S -1.7e+308 1.7e+308 The file extension determines the output file when saving a Raster* object to disk. For example, the .tif extension will create a GeoTIFF file: writeRaster(x = single_layer, filename = &quot;my_raster.tif&quot;, datatype = &quot;INT2U&quot;) The raster file format (native to the raster package) is used when a file extension is invalid or missing. Some raster file formats come with additional options. You can use them with the options parameter27. For example, GeoTIFF allows to compress the output raster with the COMPRESS option28: writeRaster(x = single_layer, filename = &quot;my_raster.tif&quot;, datatype = &quot;INT2U&quot;, options = c(&quot;COMPRESS=DEFLATE&quot;), overwrite = TRUE) Note that writeFormats() returns a list with all supported file formats on your computer. 5.6 Visual outputs R supports many different static and interactive graphics formats. The most general method to save a static plot is to open a graphic device, create a plot, and close it, for example: png(filename = &quot;lifeExp.png&quot;, width = 500, height = 350) plot(world[&quot;lifeExp&quot;]) dev.off() Other available graphic devices include pdf(), bmp(), jpeg(), png(), and tiff(). You can specify several properties of the output plot, including width, height and resolution. Additionally, several graphic packages provide its own function to save a graphical output. For example, the tmap package has the save_tmap() function. You can save a tmap object to different graphic formats by specifying the object name and a file path to a new graphic file. library(tmap) tmap_obj = tm_shape(world) + tm_polygons(col = &quot;lifeExp&quot;) save_tmap(tm = tmap_obj, filename = &quot;lifeExp_tmap.png&quot;) On the other hand, on can save interactive maps created in the mapview package as an HTML file or image using the mapshot() function: library(mapview) mapview_obj = mapview(world, zcol = &quot;lifeExp&quot;, legend = TRUE) mapshot(mapview_obj, file = &quot;my_interactive_map.html&quot;) 5.7 Exercises List and describe three types of vector, raster, and geodatabase formats. Name at least two differences between read_sf() and the more well-known function st_read(). Read the cycle_hire_xy.csv file from the spData package (Hint: it is located in the misc\\ folder). What is a geometry type of the loaded object? Download the borders of Germany using rnaturalearth, and create a new object called germany_borders. Write this new object to a file of the GeoPackage format. Download the global monthly minimum temperature with a spatial resolution of five minutes using the raster package. Extract the June values, and save them to a file named tmin_june.tif file (Hint: use raster::subset()). Create a static map of Germany’s borders, and save it to a PNG file. Create an interactive map using data from the cycle_hire_xy.csv file. Export this map to a file called cycle_hire.html. References "],
["transform.html", "6 Reprojections and Transformations Prerequisites 6.1 Introduction 6.2 CRS transformation", " 6 Reprojections and Transformations Prerequisites This chapter requires the packages tidyverse, sf, raster: library(tidyverse) library(sf) library(raster) It also relies on spData and spDataLarge, which load cycle_hire_osm dataset and provide external files: library(spData) library(spDataLarge) 6.1 Introduction As stated in Chapter 2.3, it is important to understand which CRS you are working in when undertaking spatial operations. Many spatial operations assume that you are using a projected CRS (on a Euclidean grid with units of meters rather than a geographic ‘lat/lon’ grid with units of degrees). The GEOS engine underlying most spatial operations in sf, for example, assumes your data is in a projected CRS. For this reason sf contains a function for checking if geometries have a geographic or projected CRS. This is illustrated below using the example of London: london = st_sf(geometry = st_sfc(st_point(c(-0.1, 51.5)))) st_is_longlat(london) #&gt; [1] NA The results show that when geographic data is created from scratch, or is loaded from a source that has no CRS metadata, the CRS is unspecified by default. CRS can be set with the st_set_crs function:29 london = st_set_crs(london, 4326) st_is_longlat(london) #&gt; [1] TRUE Spatial operations on objects without a CRS run on the implicit assumption that they are projected, even when in reality they are not. This can be seen by creating a buffer of one degree around the london point: london_buff = st_buffer(london, dist = 1) #&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs): st_buffer does #&gt; not correctly buffer longitude/latitude data #&gt; dist is assumed to be in decimal degrees (arc_degrees). As a result a warning message is emitted to warn the user that the operation may not work correctly and that, if the operation was intended, the distance should be in degrees (not meters or some other Euclidean distance measurement). The seemingly small difference in setting the CRS may seem inconsequential but it can have a huge impact. This is illustrated in Figure 6.1, which shows how the buffer created in the geographic CRS is dramatically elongated in the north-south direction due to the thinning of the vertical lines of longitude towards the Earth’s poles. plot(london_buff, graticule = st_crs(4326), axes = TRUE) plot(london, add = TRUE) Figure 6.1: Buffer on data with geographic CRS. To prevent this, we need to create a buffer based on a point in a projected CRS. For example, London has coordinates of c(530000, 180000) in British National Grid CRS (EPSG:27700): london_proj = st_sf(geometry = st_sfc(st_point(c(530000, 180000))), crs = 27700) This projected CRS has units in meters. One degree at the equator represents 111,319.9 meters and we can use this value to create our buffer: london_proj_buff = st_buffer(london_proj, 111319.9) The result in Figure 6.2 shows that buffers based on a projected CRS are not distorted and we can expect the same distance from our point to every part of the buffer’s border. plot(london_proj_buff, graticule = st_crs(27700), axes = TRUE) plot(london_proj, add = TRUE) Figure 6.2: Buffer on data with projected CRS. 6.2 CRS transformation While CRSs can be set manually, it is more common in real world applications to transform a known CRS into another. CRS transformation could be vital to obtain proper results in many cases. A typical example is when geometry data is provided in a geographic CRS but you want to do spatial operations, which require it to be in a projected CRS. It includes distance measurements or area calculations. CRS also represent spatial relationship between datasets. Therefore, spatial operations on many datasets can only be correctly performed when all the data have the same CRS. The most common reason to unify the CRS is to combine different datasets (e.g. merge two rasters) or apply methods which need at least two objects (e.g spatial subsetting or raster map algebra). Let’s use real-world examples to illustrate this. 6.2.1 Vector data Vector data on the most basic level is represented by individual points, and points create more complex objects, such as lines and polygons. Spatial reprojection of vectors is a mathematical transformation of coordinates of these point. Depending on projections used, reprojection could be either lossy or lossless. For example, loss of spatial information could occur when the new CRS is only adequate for smaller area than input vector. The precision could be also lost when transformation is between coordinate systems that have different datum - in those situations approximations are used. However, in most cases CRS vector transformation is lossless. The dataset cycle_hire_osm represents all cycle hire locations across London, taken from OpenStreetMap (OSM). It is automatically loaded by the spData package, meaning we do not have to load it, and its CRS can be queried as follows: st_crs(cycle_hire_osm) #&gt; Coordinate Reference System: #&gt; EPSG: 4326 #&gt; proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; CRS in R can be described as an epsg code or a proj4string definition, as described in section 2.3.3. Let’s create a new version of cycle_hire_osm in a projected CRS, using the epsg number of 27700: cycle_hire_projected = st_transform(cycle_hire_osm, 27700) st_crs(cycle_hire_projected) #&gt; Coordinate Reference System: #&gt; EPSG: 27700 #&gt; proj4string: &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs&quot; Note that the result shows that the epsg has been updated and that proj4string element of the CRS now contains, among other things +proj=tmerc (meaning it is a projected CRS using the tranverse Mercator projection) and +units=m (meaning the units of the coordinates are meters). Another function, from the rgdal library, provides a note containing the name of the CRS: crs_codes = rgdal::make_EPSG()[1:2] dplyr::filter(crs_codes, code == 27700) #&gt; code note #&gt; 1 27700 # OSGB 1936 / British National Grid The result shows that the EPSG code 27700 represents the British National Grid, a result that could have been found by searching online for “CRS 27700”. The formula that converts a geographic point into a point on the surface of the Earth is provided by the proj4string element of the crs (see proj4.org for further details): st_crs(27700)$proj4string #&gt; [1] &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs&quot; The EPSG code can be found inside the crs attribute of the object’s geometry. It is hidden from view for most of the time except when the object is printed but can be can identified and set using the st_crs function, for example st_crs(cycle_hire_osm)$epsg. 6.2.2 Raster data The basic concepts of CRS apply to both vector and raster data model. However, there are important differences in reprojection of vectors and rasters. Transformation of CRS in vector data changes coordinates of each vertex. This do not apply to raster data. Rasters are are composed of rectangular cells of the same size (expressed by map units, such as degrees or meters). To preserve this property, it is impossible to transform coordinates of cells separately. This entails that a new raster could have a different number of columns and rows, and therefore different number of cells that the original one. Therefore, values of these new cells need to be estimated after a geometric operation is completed. The projectRaster() function’s role is to reproject Raster* objects into a new object with another coordinate reference system. Compared to st_tranform(), projectRaster() only accepts the proj4string definitions. It is possible to use a EPSG code in a proj4string definition with &quot;+init=epsg:MY_NUMBER&quot;. For example, one can use the &quot;+init=epsg:4326&quot; definition to set CRS to WGS84 (EPSG code of 4326). The PROJ.4 library automaticaly adds the rest of parameters and converts it into &quot;+init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;, Let’s take a look at two examples of raster transformation - using categorical and continuous data. Land cover data are usually represented by categorical maps. The nlcd2011.tif file provides information for a small area in Utah, USA obtained from National Land Cover Database 2011 in the NAD83 / UTM zone 12N CRS. cat_raster = raster(system.file(&quot;raster/nlcd2011.tif&quot;, package = &quot;spDataLarge&quot;)) cat_raster #&gt; class : RasterLayer #&gt; dimensions : 1359, 1073, 1458207 (nrow, ncol, ncell) #&gt; resolution : 31.5, 31.5 (x, y) #&gt; extent : 301903, 335735, 4111244, 4154086 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; data source : /home/travis/R/Library/spDataLarge/raster/nlcd2011.tif #&gt; names : nlcd2011 #&gt; values : 11, 95 (min, max) In this region, 14 land cover classes were distinguished30: unique(cat_raster) #&gt; [1] 11 21 22 23 31 41 42 43 52 71 81 82 90 95 When reprojecting categorical raster, we need to ensure that our new estimated values would still have values of our original classes. This could be done using the nearest neighbor method (ngb). In this method, value of the output cell is calculated based on the nearest cell center of the input raster. For example, we want to change the CRS to WGS 84. The first step to do so is to obtain the proj4 definition of this CRS, which can be done using the http://spatialreference.org webpage. The second and last step is to define the reprojection method in the projectRaster() function, which in case of categorical data is the nearest neighbor method (ngb): wgs84 = &quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs&quot; cat_raster_wgs84 = projectRaster(cat_raster, crs = wgs84, method = &quot;ngb&quot;) cat_raster_wgs84 #&gt; class : RasterLayer #&gt; dimensions : 1394, 1111, 1548734 (nrow, ncol, ncell) #&gt; resolution : 0.000356, 0.000284 (x, y) #&gt; extent : -113, -113, 37.1, 37.5 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0 #&gt; data source : in memory #&gt; names : nlcd2011 #&gt; values : 11, 95 (min, max) Many properties of the new object differs from the previous one, which include the number of columns and rows (and therefore number of cells), resolution (transformed from meters into degrees), and extent. In the same time, it keeps the same land cover classes - unique(cat_raster_wgs84). This process of reprojection is almost identical for continuous data. The srtm.tif file contains digital elevation model for the same area in Utah from the Shuttle Radar Topography Mission (SRTM). Each value in this raster represents elevation measured in meters. con_raster = raster(system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;)) con_raster #&gt; class : RasterLayer #&gt; dimensions : 463, 459, 212517 (nrow, ncol, ncell) #&gt; resolution : 73.7, 92.5 (x, y) #&gt; extent : 301929, 335757, 4111262, 4154089 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; data source : /home/travis/R/Library/spDataLarge/raster/srtm.tif #&gt; names : srtm #&gt; values : 1050, 2895 (min, max) The nearest neighbor method should not be used for continuous raster data, as we want to preserve gradual changes in values. Alternatively, continuous data could be reprojected in the raster package using the bilinear method. In this technique, value of the output cell is calculated based on four nearest cells in the original raster. The new value is a weighted average of values from these four cells, adjusted for their distance from the center of the output cell. con_raster_wgs84 = projectRaster(con_raster, crs = wgs84, method = &quot;bilinear&quot;) con_raster_wgs84 #&gt; class : RasterLayer #&gt; dimensions : 481, 482, 231842 (nrow, ncol, ncell) #&gt; resolution : 0.000831, 0.000833 (x, y) #&gt; extent : -113, -113, 37.1, 37.5 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0 #&gt; data source : in memory #&gt; names : srtm #&gt; values : 1052, 2898 (min, max) Reprojection of continuous rasters also change spatial properties, such as the number of cells, resolution, and extent. It also slightly modifies values in the new raster, which can be seen by comparing the outputs of the summary() function between con_raster and con_raster_wgs84. summary(con_raster) summary(con_raster_wgs84) 6.2.3 Exercises CRS could be also added when creating the object with the following command: st_sf(geometry = st_sfc(st_point(c(-0.1, 51.5))), crs = 4326)↩ Full list of NLCD2011 land cover classes can be found at https://www.mrlc.gov/nlcd11_leg.php↩ "],
["references.html", "7 References", " 7 References "]
]
